{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3a294cf",
   "metadata": {},
   "source": [
    "### WINDOWING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8b76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creamos una SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734dd9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- department: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|James        |Sales     |3000  |\n",
      "|Michael      |Sales     |4600  |\n",
      "|Robert       |Sales     |4100  |\n",
      "|Maria        |Finance   |3000  |\n",
      "|James        |Sales     |3000  |\n",
      "|Scott        |Finance   |3300  |\n",
      "|Jen          |Finance   |3900  |\n",
      "|Jeff         |Marketing |3000  |\n",
      "|Kumar        |Marketing |2000  |\n",
      "|Saif         |Sales     |4100  |\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Creamos un DF\n",
    "simpleData = ((\"James\", \"Sales\", 3000), \\\n",
    "    (\"Michael\", \"Sales\", 4600),  \\\n",
    "    (\"Robert\", \"Sales\", 4100),   \\\n",
    "    (\"Maria\", \"Finance\", 3000),  \\\n",
    "    (\"James\", \"Sales\", 3000),    \\\n",
    "    (\"Scott\", \"Finance\", 3300),  \\\n",
    "    (\"Jen\", \"Finance\", 3900),    \\\n",
    "    (\"Jeff\", \"Marketing\", 3000), \\\n",
    "    (\"Kumar\", \"Marketing\", 2000),\\\n",
    "    (\"Saif\", \"Sales\", 4100) \\\n",
    "  ) #Datos\n",
    " \n",
    "columns= [\"employee_name\", \"department\", \"salary\"]#nombre columnas\n",
    "df = spark.createDataFrame(data = simpleData, schema = columns)#crear DF\n",
    "df.printSchema()#mostrar esquema\n",
    "df.show(truncate=False)#mostrar datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94e35a",
   "metadata": {},
   "source": [
    "**row_number Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bca0fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f263b342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|row_number|\n",
      "+-------------+----------+------+----------+\n",
      "|James        |Sales     |3000  |1         |\n",
      "|James        |Sales     |3000  |2         |\n",
      "|Robert       |Sales     |4100  |3         |\n",
      "|Saif         |Sales     |4100  |4         |\n",
      "|Michael      |Sales     |4600  |5         |\n",
      "|Maria        |Finance   |3000  |1         |\n",
      "|Scott        |Finance   |3300  |2         |\n",
      "|Jen          |Finance   |3900  |3         |\n",
      "|Kumar        |Marketing |2000  |1         |\n",
      "|Jeff         |Marketing |3000  |2         |\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(\"salary\") #particionamos por departamento y ordenamos por salario\n",
    "df.withColumn(\"row_number\",row_number().over(windowSpec)).show(truncate=False) #numeramos las filas por departamento con salario ascendente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25448dcb",
   "metadata": {},
   "source": [
    "**rank Window Function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aebdb79",
   "metadata": {},
   "source": [
    "Esta funcion asigna un ranking por departamento en base al salario de menor a mayor. Si hay empate asigna el mismo puesto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bdaaeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|rank|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|   1|\n",
      "|        James|     Sales|  3000|   1|\n",
      "|       Robert|     Sales|  4100|   3|\n",
      "|         Saif|     Sales|  4100|   3|\n",
      "|      Michael|     Sales|  4600|   5|\n",
      "|        Maria|   Finance|  3000|   1|\n",
      "|        Scott|   Finance|  3300|   2|\n",
      "|          Jen|   Finance|  3900|   3|\n",
      "|        Kumar| Marketing|  2000|   1|\n",
      "|         Jeff| Marketing|  3000|   2|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import rank\n",
    "df.withColumn(\"rank\",rank().over(windowSpec)).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2260e0b",
   "metadata": {},
   "source": [
    "**dense_rank Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa671e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----------+\n",
      "|employee_name|department|salary|dense_rank|\n",
      "+-------------+----------+------+----------+\n",
      "|        James|     Sales|  3000|         1|\n",
      "|        James|     Sales|  3000|         1|\n",
      "|       Robert|     Sales|  4100|         2|\n",
      "|         Saif|     Sales|  4100|         2|\n",
      "|      Michael|     Sales|  4600|         3|\n",
      "|        Maria|   Finance|  3000|         1|\n",
      "|        Scott|   Finance|  3300|         2|\n",
      "|          Jen|   Finance|  3900|         3|\n",
      "|        Kumar| Marketing|  2000|         1|\n",
      "|         Jeff| Marketing|  3000|         2|\n",
      "+-------------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import dense_rank\n",
    "df.withColumn(\"dense_rank\",dense_rank().over(windowSpec)).show() #igual que el anterior pero no se salta puestos cuando hay empates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc19c5a1",
   "metadata": {},
   "source": [
    "**percent_rank Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f62ee85a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------+\n",
      "|employee_name|department|salary|percent_rank|\n",
      "+-------------+----------+------+------------+\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|        James|     Sales|  3000|         0.0|\n",
      "|       Robert|     Sales|  4100|         0.5|\n",
      "|         Saif|     Sales|  4100|         0.5|\n",
      "|      Michael|     Sales|  4600|         1.0|\n",
      "|        Maria|   Finance|  3000|         0.0|\n",
      "|        Scott|   Finance|  3300|         0.5|\n",
      "|          Jen|   Finance|  3900|         1.0|\n",
      "|        Kumar| Marketing|  2000|         0.0|\n",
      "|         Jeff| Marketing|  3000|         1.0|\n",
      "+-------------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import percent_rank\n",
    "df.withColumn(\"percent_rank\",percent_rank().over(windowSpec)).show() #igual que la anterior pero asignando \"porcentajes\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51625d5f",
   "metadata": {},
   "source": [
    "**ntile Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2bcfa5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+-----+\n",
      "|employee_name|department|salary|ntile|\n",
      "+-------------+----------+------+-----+\n",
      "|        James|     Sales|  3000|    1|\n",
      "|        James|     Sales|  3000|    1|\n",
      "|       Robert|     Sales|  4100|    1|\n",
      "|         Saif|     Sales|  4100|    2|\n",
      "|      Michael|     Sales|  4600|    2|\n",
      "|        Maria|   Finance|  3000|    1|\n",
      "|        Scott|   Finance|  3300|    1|\n",
      "|          Jen|   Finance|  3900|    2|\n",
      "|        Kumar| Marketing|  2000|    1|\n",
      "|         Jeff| Marketing|  3000|    2|\n",
      "+-------------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import ntile\n",
    "df.withColumn(\"ntile\",ntile(2).over(windowSpec)).show() #te devuelve un rango relativo entre 1 y el valor de la funcion ntile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c5f35e",
   "metadata": {},
   "source": [
    "**cume_dist Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74ffe34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------------------+\n",
      "|employee_name|department|salary|         cume_dist|\n",
      "+-------------+----------+------+------------------+\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|        James|     Sales|  3000|               0.4|\n",
      "|       Robert|     Sales|  4100|               0.8|\n",
      "|         Saif|     Sales|  4100|               0.8|\n",
      "|      Michael|     Sales|  4600|               1.0|\n",
      "|        Maria|   Finance|  3000|0.3333333333333333|\n",
      "|        Scott|   Finance|  3300|0.6666666666666666|\n",
      "|          Jen|   Finance|  3900|               1.0|\n",
      "|        Kumar| Marketing|  2000|               0.5|\n",
      "|         Jeff| Marketing|  3000|               1.0|\n",
      "+-------------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import cume_dist    \n",
    "df.withColumn(\"cume_dist\",cume_dist().over(windowSpec)).show() #distribucion acumulada de los valores:\n",
    "#en sales hay 5 valores, por lo que a cada uno le corresponde 0.2, como hay 4 valores que se repiten 2 a 2 se suma\n",
    "#en finance hay tres valores, por lo que le corresponde 0.3 periodo a cada uno\n",
    "#en marketing hay dos valores, por lo que corresponde 0.5 a cada uno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff13e1e",
   "metadata": {},
   "source": [
    "**lag Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "519c22af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary| lag|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|null|\n",
      "|        James|     Sales|  3000|null|\n",
      "|       Robert|     Sales|  4100|3000|\n",
      "|         Saif|     Sales|  4100|3000|\n",
      "|      Michael|     Sales|  4600|4100|\n",
      "|        Maria|   Finance|  3000|null|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|3000|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lag    \n",
    "df.withColumn(\"lag\",lag(\"salary\",2).over(windowSpec)).show() #devuelve valores anteriores con el lag establecido, en este caso \n",
    "#devuelve el valor de dos filas anteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bcf57d",
   "metadata": {},
   "source": [
    "**lead Window Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab2382ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+----+\n",
      "|employee_name|department|salary|lead|\n",
      "+-------------+----------+------+----+\n",
      "|        James|     Sales|  3000|4100|\n",
      "|        James|     Sales|  3000|4100|\n",
      "|       Robert|     Sales|  4100|4600|\n",
      "|         Saif|     Sales|  4100|null|\n",
      "|      Michael|     Sales|  4600|null|\n",
      "|        Maria|   Finance|  3000|3900|\n",
      "|        Scott|   Finance|  3300|null|\n",
      "|          Jen|   Finance|  3900|null|\n",
      "|        Kumar| Marketing|  2000|null|\n",
      "|         Jeff| Marketing|  3000|null|\n",
      "+-------------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lead    \n",
    "df.withColumn(\"lead\",lead(\"salary\",2).over(windowSpec)).show() #devuelve los valores posteriores con el lead establecido, en este\n",
    "#caso devuelve el valor de las dos filas posteriores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce2eb6",
   "metadata": {},
   "source": [
    "**Window Aggregate Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfed088b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+------+-----+----+----+\n",
      "|employee_name|department|salary|row|   avg|  sum| min| max|\n",
      "+-------------+----------+------+---+------+-----+----+----+\n",
      "|        James|     Sales|  3000|  1|3760.0|18800|3000|4600|\n",
      "|        Maria|   Finance|  3000|  1|3400.0|10200|3000|3900|\n",
      "|        Kumar| Marketing|  2000|  1|2500.0| 5000|2000|3000|\n",
      "+-------------+----------+------+---+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpecAgg  = Window.partitionBy(\"department\") #particionamos por departamento\n",
    "\n",
    "from pyspark.sql.functions import col,avg,sum,min,max,row_number #cargamos las funciones\n",
    "\n",
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .where(col(\"row\")==1).select(\"*\") \\\n",
    "  .show()\n",
    "#en este caso lo que hace es coger la columna salario y la agrega por departamento, devolviendo asi las distintas metricas pedidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8295a189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---+------+-----+----+----+\n",
      "|employee_name|department|salary|row|   avg|  sum| min| max|\n",
      "+-------------+----------+------+---+------+-----+----+----+\n",
      "|        James|     Sales|  3000|  1|3760.0|18800|3000|4600|\n",
      "|        James|     Sales|  3000|  2|3760.0|18800|3000|4600|\n",
      "|       Robert|     Sales|  4100|  3|3760.0|18800|3000|4600|\n",
      "|         Saif|     Sales|  4100|  4|3760.0|18800|3000|4600|\n",
      "|      Michael|     Sales|  4600|  5|3760.0|18800|3000|4600|\n",
      "|        Maria|   Finance|  3000|  1|3400.0|10200|3000|3900|\n",
      "|        Scott|   Finance|  3300|  2|3400.0|10200|3000|3900|\n",
      "|          Jen|   Finance|  3900|  3|3400.0|10200|3000|3900|\n",
      "|        Kumar| Marketing|  2000|  1|2500.0| 5000|2000|3000|\n",
      "|         Jeff| Marketing|  3000|  2|2500.0| 5000|2000|3000|\n",
      "+-------------+----------+------+---+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"row\",row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", avg(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", sum(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", min(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", max(col(\"salary\")).over(windowSpecAgg)) \\\n",
    "  .select(\"*\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af85f818",
   "metadata": {},
   "source": [
    "**Hacer con JOIN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34681145",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+----+----+\n",
      "|department|  mean|  sum| min| max|\n",
      "+----------+------+-----+----+----+\n",
      "|     Sales|3760.0|18800|3000|4600|\n",
      "|   Finance|3400.0|10200|3000|3900|\n",
      "| Marketing|2500.0| 5000|2000|3000|\n",
      "+----------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "dfjoin = df.groupBy(\"department\")\\\n",
    "           .agg(F.avg(\"salary\").alias(\"mean\"),\n",
    "                F.sum(\"salary\").alias(\"sum\"),\n",
    "                F.min(\"salary\").alias(\"min\"),\n",
    "                F.max(\"salary\").alias(\"max\"))\n",
    "\n",
    "dfjoin.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd028762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+------+-----+----+----+\n",
      "|employee_name|department|salary|  mean|  sum| max| min|\n",
      "+-------------+----------+------+------+-----+----+----+\n",
      "|        James|     Sales|  3000|3760.0|18800|4600|3000|\n",
      "|      Michael|     Sales|  4600|3760.0|18800|4600|3000|\n",
      "|       Robert|     Sales|  4100|3760.0|18800|4600|3000|\n",
      "|        James|     Sales|  3000|3760.0|18800|4600|3000|\n",
      "|         Saif|     Sales|  4100|3760.0|18800|4600|3000|\n",
      "|        Maria|   Finance|  3000|3400.0|10200|3900|3000|\n",
      "|        Scott|   Finance|  3300|3400.0|10200|3900|3000|\n",
      "|          Jen|   Finance|  3900|3400.0|10200|3900|3000|\n",
      "|         Jeff| Marketing|  3000|2500.0| 5000|3000|2000|\n",
      "|        Kumar| Marketing|  2000|2500.0| 5000|3000|2000|\n",
      "+-------------+----------+------+------+-----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.alias(\"df1\")\n",
    "df2 = dfjoin.alias(\"df2\")\n",
    "df_final = df1.join(df2, (df1.department == df2.department), how=\"left\"). select(df1[\"*\"], df2[\"mean\"], df2[\"sum\"],df2[\"max\"], df2[\"min\"])\n",
    "df_final.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31926bc9",
   "metadata": {},
   "source": [
    "### PIVOTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88753e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Amount: long (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n",
      "+-------+------+-------+\n",
      "|Product|Amount|Country|\n",
      "+-------+------+-------+\n",
      "|Banana |1000  |USA    |\n",
      "|Carrots|1500  |USA    |\n",
      "|Beans  |1600  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Orange |2000  |USA    |\n",
      "|Banana |400   |China  |\n",
      "|Carrots|1200  |China  |\n",
      "|Beans  |1500  |China  |\n",
      "|Orange |4000  |China  |\n",
      "|Banana |2000  |Canada |\n",
      "|Carrots|2000  |Canada |\n",
      "|Beans  |2000  |Mexico |\n",
      "+-------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Banana\",1000,\"USA\"), (\"Carrots\",1500,\"USA\"), (\"Beans\",1600,\"USA\"), \\\n",
    "      (\"Orange\",2000,\"USA\"),(\"Orange\",2000,\"USA\"),(\"Banana\",400,\"China\"), \\\n",
    "      (\"Carrots\",1200,\"China\"),(\"Beans\",1500,\"China\"),(\"Orange\",4000,\"China\"), \\\n",
    "      (\"Banana\",2000,\"Canada\"),(\"Carrots\",2000,\"Canada\"),(\"Beans\",2000,\"Mexico\")] #Creamos los datos\n",
    "\n",
    "columns= [\"Product\",\"Amount\",\"Country\"] #nombre a las columnas\n",
    "df = spark.createDataFrame(data = data, schema = columns) #creamos DF\n",
    "df.printSchema() #mostramos esquema\n",
    "df.show(truncate=False)#mostramos DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fae6696e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Product: string (nullable = true)\n",
      " |-- Canada: long (nullable = true)\n",
      " |-- China: long (nullable = true)\n",
      " |-- Mexico: long (nullable = true)\n",
      " |-- USA: long (nullable = true)\n",
      "\n",
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Utilizamos los valores de la columna country para formar nuevas columnas y agrupamos los datos por la suma de la cantidad\n",
    "pivotDF=df.groupBy(\"Product\").pivot(\"Country\").sum(\"Amount\")\n",
    "pivotDF.printSchema()\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58c3f983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+------+------+\n",
      "|Product|USA |China|Canada|Mexico|\n",
      "+-------+----+-----+------+------+\n",
      "|Orange |4000|4000 |null  |null  |\n",
      "|Beans  |1600|1500 |null  |2000  |\n",
      "|Banana |1000|400  |2000  |null  |\n",
      "|Carrots|1500|1200 |2000  |null  |\n",
      "+-------+----+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Otra forma de hacerlo\n",
    "countries = [\"USA\",\"China\",\"Canada\",\"Mexico\"]\n",
    "pivotDF = df.groupBy(\"Product\").pivot(\"Country\", countries).sum(\"Amount\")\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aba08f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+-----+------+----+\n",
      "|Product|Canada|China|Mexico|USA |\n",
      "+-------+------+-----+------+----+\n",
      "|Orange |null  |4000 |null  |4000|\n",
      "|Beans  |null  |1500 |2000  |1600|\n",
      "|Banana |2000  |400  |null  |1000|\n",
      "|Carrots|2000  |1200 |null  |1500|\n",
      "+-------+------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Otra forma a traves de dos agrupaciones\n",
    "pivotDF = df.groupBy(\"Product\",\"Country\") \\\n",
    "      .sum(\"Amount\") \\\n",
    "      .groupBy(\"Product\") \\\n",
    "      .pivot(\"Country\") \\\n",
    "      .sum(\"sum(Amount)\")\n",
    "\n",
    "pivotDF.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7566df",
   "metadata": {},
   "source": [
    "### UNPIVOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fddac4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----+\n",
      "|Product|Country|Total|\n",
      "+-------+-------+-----+\n",
      "|Orange |China  |4000 |\n",
      "|Beans  |China  |1500 |\n",
      "|Beans  |Mexico |2000 |\n",
      "|Banana |Canada |2000 |\n",
      "|Banana |China  |400  |\n",
      "|Carrots|Canada |2000 |\n",
      "|Carrots|China  |1200 |\n",
      "+-------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "unpivotExpr = \"stack(3, 'Canada', Canada, 'China', China, 'Mexico', Mexico) as (Country,Total)\"\n",
    "\n",
    "unPivotDF = pivotDF.select(\"Product\", expr(unpivotExpr)) \\\n",
    "    .where(\"Total is not null\")\n",
    "\n",
    "unPivotDF.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
