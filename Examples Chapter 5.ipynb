{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc754671",
   "metadata": {},
   "source": [
    "**UDFs**(User Defined Functions): además de las funciones que hay predefinidas en spark, el usuario puede definir funciones que aplicar a un conjunto de datos una vez definida la función. Las funciones solo existen mientras se esta activa la sesion, una vez cerrada se eliminan, igual que las variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e6e57b",
   "metadata": {},
   "source": [
    "##### Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42509fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002836.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1623753172799)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "cubed: Long => Long = $Lambda$1985/0x0000000801426428@444394e6\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Ejemplo UDF\n",
    "val cubed = (s: Long) => {\n",
    " s * s * s\n",
    "}\n",
    "\n",
    "//Guarrdamos la funcion\n",
    "spark.udf.register(\"cubed\", cubed)\n",
    "\n",
    "//Creamos un rango de valores al que aplicar la función\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121ef9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Aplicamos la función \n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11902a7e",
   "metadata": {},
   "source": [
    "##### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8767e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "980f02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.cubed(s)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Definimos la funcion\n",
    "def cubed(s):\n",
    "     return s * s * s\n",
    "\n",
    "##Registramos la funcion para poder utilizarla\n",
    "spark.udf.register(\"cubed\", cubed, LongType())\n",
    "\n",
    "##Creamos un rango de valores\n",
    "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "922f3eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id|id_cubed|\n",
      "+---+--------+\n",
      "|  1|       1|\n",
      "|  2|       8|\n",
      "|  3|      27|\n",
      "|  4|      64|\n",
      "|  5|     125|\n",
      "|  6|     216|\n",
      "|  7|     343|\n",
      "|  8|     512|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Aplicamos la función\n",
    "spark.sql(\"SELECT id, cubed(id) AS id_cubed FROM udf_test\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9c5e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-4.0.1-cp38-cp38-win_amd64.whl (13.3 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\nerea.gomez\\anaconda3\\lib\\site-packages (from pyarrow) (1.20.1)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "22c4dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Ejemplo utilizando Pandas UDF\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import LongType\n",
    "\n",
    "##Declaramos la funcion\n",
    "def cubed2(a: pd.Series) -> pd.Series:\n",
    "     return a * a * a\n",
    "\n",
    "#Creamos la funcion cube con pandas UDF\n",
    "cubed_udf = pandas_udf(cubed, returnType=LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "158c6698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     1\n",
      "1     8\n",
      "2    27\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Definimos una serie a la que aplicar la funcion y la aplicamos\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(cubed(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb0f4314",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "| id|cubed(id)|\n",
      "+---+---------+\n",
      "|  1|        1|\n",
      "|  2|        8|\n",
      "|  3|       27|\n",
      "+---+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Ahora lo hacemos para un Spark DF\n",
    "df = spark.range(1, 4)\n",
    "df.select(\"id\", cubed_udf(col(\"id\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa51204",
   "metadata": {},
   "source": [
    "### Using the Spark SQL Shell    \n",
    "      \n",
    "``cd C:\\Spark\\bin``        \n",
    "Iniciamos la spark-sql:      \n",
    "``spark-sql``     \n",
    "Creamos una tabla:       \n",
    "`CREATE TABLE people (name STRING, age int);`      \n",
    "Antes de insertar valores, salimos de la spark-shell y ponemos el siguiente comando:         \n",
    "``winutils chmod -R 777 spark-warehouse``        \n",
    "Una vez hecho esto, volvemos a spark-sql.        \n",
    "Insertamos los datos en la tabla:        \n",
    "`INSERT INTO people VALUES (\"Michael\", NULL);\n",
    "INSERT INTO people VALUES (\"Andy\", 30);\n",
    "INSERT INTO people VALUES (\"Samantha\", 19);`        \n",
    "Mostramos las tablas que tenemos guardadas:        \n",
    "``SHOW TABLES;``      \n",
    "Hacemos querys:    \n",
    "``SELECT * FROM people WHERE age < 20;``     \n",
    "``SELECT name FROM people WHERE age IS NULL;``    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d8cac9",
   "metadata": {},
   "source": [
    "### External Data Sources    \n",
    "**PostgreSQL**       \n",
    "*Scala*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5789dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002836.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1623827372967)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@b041b74\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    ".builder()\n",
    ".appName(\"ClientPL\")\n",
    ".master(\"local\")\n",
    ".enableHiveSupport()\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09afd4f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jdbcDF1: org.apache.spark.sql.DataFrame = [date_received: string, product_name: string ... 16 more fields]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Option 1: Loading data from a JDBC source using load method\n",
    "val jdbcDF1 = spark\n",
    " .read\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/consumer complaints\")\n",
    " .option(\"dbtable\", \"public.consumer_complaints\")\n",
    " .option(\"user\", \"postgres\")\n",
    " .option(\"password\", \"Ttzz2r2a\")\n",
    " .option(\"driver\", \"org.postgresql.Driver\")\n",
    " .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14adb25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF1.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8a8b44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: Array[String] = Array(date_received, product_name, sub_product, issue, sub_issue, consumer_complaint_narrative, company_public_response, company, state_name, zip_code, tags, consumer_consent_provided, submitted_via, date_sent, company_response_to_consumer, timely_response, consumer_disputed, complaint_id)\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdbcDF1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f2a120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|date_received|        product_name|         sub_product|               issue|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|   2013-07-29|       Consumer Loan|        Vehicle loan|Managing the loan...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Using a debit or ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Account opening, ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Deposits and with...|\n",
      "|   2013-07-29|            Mortgage|Conventional fixe...|Loan servicing, p...|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "jdbcDF1.select(\"date_received\", \"product_name\", \"sub_product\", \"issue\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1633885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import java.util.Properties\r\n",
       "cxnProp: java.util.Properties = {password=Ttzz2r2a, user=postgres}\r\n",
       "jdbcDF2: org.apache.spark.sql.DataFrame = [date_received: string, product_name: string ... 16 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Read Option 2: Loading data from a JDBC source using jdbc method\n",
    "// Create connection properties\n",
    "import java.util.Properties\n",
    "val cxnProp = new Properties()\n",
    "cxnProp.put(\"user\", \"postgres\")\n",
    "cxnProp.put(\"password\", \"Ttzz2r2a\")\n",
    "// Load data using the connection properties\n",
    "val jdbcDF2 = spark\n",
    " .read\n",
    " .jdbc(\"jdbc:postgresql://localhost:5432/consumer complaints\", \"public.consumer_complaints\", cxnProp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97db6a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|date_received|        product_name|         sub_product|               issue|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|   2013-07-29|       Consumer Loan|        Vehicle loan|Managing the loan...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Using a debit or ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Account opening, ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Deposits and with...|\n",
      "|   2013-07-29|            Mortgage|Conventional fixe...|Loan servicing, p...|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "jdbcDF2.select(\"date_received\", \"product_name\", \"sub_product\", \"issue\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d75050ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Write Option 1: Saving data to a JDBC source using save method\n",
    "jdbcDF1\n",
    " .write\n",
    " .format(\"jdbc\")\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/consumer complaints\")\n",
    " .option(\"dbtable\", \"public.consumer_complaints2\")\n",
    " .option(\"user\", \"postgres\")\n",
    " .option(\"password\", \"Ttzz2r2a\")\n",
    " .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0abb9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdbcDF2.write\n",
    " .jdbc(s\"jdbc:postgresql://localhost:5432/consumer complaints\", \"public.consumer_complaints3\", cxnProp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fede2434",
   "metadata": {},
   "source": [
    "*Python*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "520974e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Option 1: Loading data from a JDBC source using load method\n",
    "jdbcDF1 = (spark\\\n",
    " .read\\\n",
    " .format(\"jdbc\")\\\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/consumer complaints\")\\\n",
    " .option(\"dbtable\", \"public.consumer_complaints\")\\\n",
    " .option(\"user\", \"postgres\")\\\n",
    " .option(\"password\", \"Ttzz2r2a\")\\\n",
    " .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217a7709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date_received',\n",
       " 'product_name',\n",
       " 'sub_product',\n",
       " 'issue',\n",
       " 'sub_issue',\n",
       " 'consumer_complaint_narrative',\n",
       " 'company_public_response',\n",
       " 'company',\n",
       " 'state_name',\n",
       " 'zip_code',\n",
       " 'tags',\n",
       " 'consumer_consent_provided',\n",
       " 'submitted_via',\n",
       " 'date_sent',\n",
       " 'company_response_to_consumer',\n",
       " 'timely_response',\n",
       " 'consumer_disputed',\n",
       " 'complaint_id']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jdbcDF1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35a67e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|date_received|        product_name|         sub_product|               issue|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|   2013-07-29|       Consumer Loan|        Vehicle loan|Managing the loan...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Using a debit or ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Account opening, ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Deposits and with...|\n",
      "|   2013-07-29|            Mortgage|Conventional fixe...|Loan servicing, p...|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcDF1.select(\"date_received\", \"product_name\", \"sub_product\", \"issue\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c6a99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Option 2: Loading data from a JDBC source using jdbc method\n",
    "jdbcDF2 = (spark\\\n",
    " .read\\\n",
    " .jdbc(\"jdbc:postgresql://localhost:5432/consumer complaints\", \"public.consumer_complaints\",\\\n",
    " properties={\"user\": \"postgres\", \"password\": \"Ttzz2r2a\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c95bf27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|date_received|        product_name|         sub_product|               issue|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "|   2013-07-29|       Consumer Loan|        Vehicle loan|Managing the loan...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Using a debit or ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Account opening, ...|\n",
      "|   2013-07-29|Bank account or s...|    Checking account|Deposits and with...|\n",
      "|   2013-07-29|            Mortgage|Conventional fixe...|Loan servicing, p...|\n",
      "+-------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdbcDF2.select(\"date_received\", \"product_name\", \"sub_product\", \"issue\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "343f53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Option 1: Saving data to a JDBC source using save method\n",
    "(jdbcDF1\n",
    " .write\\\n",
    " .format(\"jdbc\")\\\n",
    " .option(\"url\", \"jdbc:postgresql://localhost:5432/consumer complaints\")\\\n",
    " .option(\"dbtable\", \"public.consumer_complaints4\")\\\n",
    " .option(\"user\", \"postgres\")\\\n",
    " .option(\"password\", \"Ttzz2r2a\")\\\n",
    " .save())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7858f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Option 2: Saving data to a JDBC source using jdbc method\n",
    "(jdbcDF2\n",
    " .write\\\n",
    " .jdbc(\"jdbc:postgresql://localhost:5432/consumer complaints\", \"public.consumer_complaints5\",\\\n",
    " properties={\"user\": \"postgres\", \"password\": \"Ttzz2r2a\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da752e1",
   "metadata": {},
   "source": [
    "#### Conexión con MySQL - Scala\n",
    "Loading data from a JDBC source using load \n",
    "\n",
    "    val jdbcDF = spark\n",
    "     .read\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "     .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "     .option(\"dbtable\", \"[TABLENAME]\")\n",
    "     .option(\"user\", \"[USERNAME]\")\n",
    "     .option(\"password\", \"[PASSWORD]\")\n",
    "     .load()\n",
    "Saving data to a JDBC source using save \n",
    "\n",
    "    jdbcDF\n",
    "     .write\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "     .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "     .option(\"dbtable\", \"[TABLENAME]\")\n",
    "     .option(\"user\", \"[USERNAME]\")\n",
    "     .option(\"password\", \"[PASSWORD]\")\n",
    "     .save()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042663ea",
   "metadata": {},
   "source": [
    "#### Conexión con MySQL - Python\n",
    "Loading data from a JDBC source using load \n",
    "\n",
    "    jdbcDF = (spark\n",
    "     .read\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "     .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "     .option(\"dbtable\", \"[TABLENAME]\")\n",
    "     .option(\"user\", \"[USERNAME]\")\n",
    "     .option(\"password\", \"[PASSWORD]\")\n",
    "     .load())\n",
    "\n",
    "Saving data to a JDBC source using save \n",
    "\n",
    "    jdbcDF\n",
    "     .write\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", \"jdbc:mysql://[DBSERVER]:3306/[DATABASE]\")\n",
    "     .option(\"driver\", \"com.mysql.jdbc.Driver\")\n",
    "     .option(\"dbtable\", \"[TABLENAME]\")\n",
    "     .option(\"user\", \"[USERNAME]\")\n",
    "     .option(\"password\", \"[PASSWORD]\")\n",
    "     .save())  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a78462",
   "metadata": {},
   "source": [
    "#### Conexión con MS SQL Server - Scala\n",
    "Loading data from a JDBC source\n",
    "Configure jdbcUrl\n",
    "\n",
    "    val jdbcUrl = \"jdbc:sqlserver://[DBSERVER]:1433;database=[DATABASE]\"\n",
    "Create a Properties() object to hold the parameters.      \n",
    "Note, you can create the JDBC URL without passing in the user/password parameters directly.\n",
    "\n",
    "    val cxnProp = new Properties()\n",
    "    cxnProp.put(\"user\", \"[USERNAME]\")\n",
    "    cxnProp.put(\"password\", \"[PASSWORD]\")\n",
    "    cxnProp.put(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\")\n",
    "Load data using the connection properties\n",
    "\n",
    "    val jdbcDF = spark.read.jdbc(jdbcUrl, \"[TABLENAME]\", cxnProp)\n",
    "Saving data to a JDBC source\n",
    "    \n",
    "    jdbcDF.write.jdbc(jdbcUrl, \"[TABLENAME]\", cxnProp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedffd24",
   "metadata": {},
   "source": [
    "#### Conexión con MS SQL Server - Python\n",
    "In Python\n",
    "Configure jdbcUrl\n",
    "\n",
    "    jdbcUrl = \"jdbc:sqlserver://[DBSERVER]:1433;database=[DATABASE]\"\n",
    "Loading data from a JDBC source\n",
    "\n",
    "    jdbcDF = (spark\n",
    "     .read\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", jdbcUrl)\n",
    "     .option(\"dbtable\", \"[TABLENAME]\")\n",
    "     .option(\"user\", \"[USERNAME]\")\n",
    "     .option(\"password\", \"[PASSWORD]\")\n",
    "     .load())\n",
    "Saving data to a JDBC source\n",
    "\n",
    "    (jdbcDF\n",
    "     .write\n",
    "     .format(\"jdbc\")\n",
    "     .option(\"url\", jdbcUrl)\n",
    "     .option(\"dbtable\", \"[TABLENAME]\")\n",
    "     .option(\"user\", \"[USERNAME]\")\n",
    "     .option(\"password\", \"[PASSWORD]\")\n",
    "     .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5eef4f",
   "metadata": {},
   "source": [
    "### Higher-Order Functions in DataFrames and Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f69d36",
   "metadata": {},
   "source": [
    "**Opcion 1**    \n",
    "Explode (crea una nueva fila para cada elemento dentro de la columna seleccionada) and Collect (devuelve una lista de objetos con duplicados)\n",
    "    \n",
    "    spark.sql(\"\"\"SELECT id, collect_list(value + 1) AS values\n",
    "    FROM (SELECT id, EXPLODE(values) AS value\n",
    "    FROM table) x\n",
    "    GROUP BY id\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2a9f2c",
   "metadata": {},
   "source": [
    "**Opcion 2**    \n",
    "User-Defined Function:  utilizando map() para recorrer cada elemento (valor) y realiza la operación de adición\n",
    "\n",
    "    spark.sql(\"SELECT id, plusOneInt(values) AS values FROM table\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f7d6e",
   "metadata": {},
   "source": [
    "**Transform():** toma un array y una función anónima como entrada. La función crea un nuevo array aplicando la función a cada elemento y luego asigna el resultado al array de salida (mas eficiente que UDFs).\n",
    "    \n",
    "    transform(values, value -> lambda expression)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812e8cc9",
   "metadata": {},
   "source": [
    "*Ejemplos*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5492708",
   "metadata": {},
   "source": [
    "*Pyhton*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e99ad548",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark =(SparkSession\n",
    "        .builder\n",
    "        .appName(\"ejemplo\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3be00c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType([StructField(\"celsius\", ArrayType(IntegerType()))]) ##Creo el esquema de los datos\n",
    "t_list = [[35, 36, 32, 30, 40, 42, 38]], [[31, 32, 34, 55, 56]] ##Creo el array\n",
    "t_c = spark.createDataFrame(t_list, schema) ##Creo el DF\n",
    "t_c.createOrReplaceTempView(\"tC\") ##Hago una vista temporal\n",
    "t_c.show() ##Muestro el DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38c96f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Funcion transform()\n",
    "spark.sql(\"\"\"SELECT celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fad93123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "35*9/5+32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169796b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Funcion filter()\n",
    "spark.sql(\"\"\"SELECT celsius, filter(celsius, t -> t > 38) as high FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62fb8097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Funcion exits()\n",
    "spark.sql(\"\"\"SELECT celsius, exists(celsius, t -> t = 38) as threshold FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e56d8626",
   "metadata": {},
   "source": [
    "##Funcion reduce()\n",
    "spark.sql(\"\"\"SELECT celsius, reduce(celsius, 0, (t, acc) -> t + acc, acc -> (acc div size(celsius) * 9 div 5) + 32) \n",
    "as avgFahrenheit FROM tC\"\"\").show()\n",
    "\n",
    "(no funciona por la conf del kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05433920",
   "metadata": {},
   "source": [
    "*Scala*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5f67d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@b041b74\r\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"ejemplo\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "588e3013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             celsius|\n",
      "+--------------------+\n",
      "|[35, 36, 32, 30, ...|\n",
      "|[31, 32, 34, 55, 56]|\n",
      "+--------------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t1: Array[Int] = Array(35, 36, 32, 30, 40, 42, 38)\r\n",
       "t2: Array[Int] = Array(31, 32, 34, 55, 56)\r\n",
       "tC: org.apache.spark.sql.DataFrame = [celsius: array<int>]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creo el array\n",
    "val t1 = Array(35, 36, 32, 30, 40, 42, 38) \n",
    "val t2 = Array(31, 32, 34, 55, 56)\n",
    "//Transformo a DF\n",
    "val tC = Seq(t1, t2).toDF(\"celsius\") \n",
    "//Vista temporal\n",
    "tC.createOrReplaceTempView(\"tC\")\n",
    "//Muestro DF\n",
    "tC.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d33bf97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             celsius|          fahrenheit|\n",
      "+--------------------+--------------------+\n",
      "|[35, 36, 32, 30, ...|[95, 96, 89, 86, ...|\n",
      "|[31, 32, 34, 55, 56]|[87, 89, 93, 131,...|\n",
      "+--------------------+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Funcion transform()\n",
    "spark.sql(\"\"\"SELECT celsius, transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cbabc31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             celsius|    high|\n",
      "+--------------------+--------+\n",
      "|[35, 36, 32, 30, ...|[40, 42]|\n",
      "|[31, 32, 34, 55, 56]|[55, 56]|\n",
      "+--------------------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Funcion filter()\n",
    "spark.sql(\"\"\"SELECT celsius, filter(celsius, t -> t > 38) as high FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3de7414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|             celsius|threshold|\n",
      "+--------------------+---------+\n",
      "|[35, 36, 32, 30, ...|     true|\n",
      "|[31, 32, 34, 55, 56]|    false|\n",
      "+--------------------+---------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Funcion exits()\n",
    "spark.sql(\"\"\"SELECT celsius, exists(celsius, t -> t = 38) as threshold FROM tC\"\"\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61136fe1",
   "metadata": {},
   "source": [
    "//Funcion reduce()\n",
    "import org.apache.spark.sql.functions.{reduce}\n",
    "spark.sql(\"\"\"SELECT celsius, reduce(celsius, 0, (t, acc) -> t + acc, acc -> (acc div size(celsius) * 9 div 5) + 32) \n",
    "as avgFahrenheit FROM tC\"\"\").show()\n",
    "\n",
    "(no funciona por la conf del kernel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e35cb2",
   "metadata": {},
   "source": [
    "### Common DataFrames and Spark SQL Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad602106",
   "metadata": {},
   "source": [
    "*Scala*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b678f3",
   "metadata": {},
   "source": [
    "**Creamos nuestra base de datos para los ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3352b80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002836.bosonit.local:4040\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1623842125218)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f38fc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delaysPath: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv\r\n",
       "airportsPath: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/airport-codes-na.txt\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Set file paths\n",
    "val delaysPath =\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv\"\n",
    "val airportsPath =\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b24f448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airports: org.apache.spark.sql.DataFrame = [City: string, State: string ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Obtain airports data set\n",
    "val airports = spark.read\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferschema\", \"true\")\n",
    " .option(\"delimiter\", \"\\t\")\n",
    " .csv(airportsPath)\n",
    "airports.createOrReplaceTempView(\"airports_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576c599a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delays: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Obtain departure Delays data set\n",
    "val delays = spark.read\n",
    " .option(\"header\",\"true\")\n",
    " .csv(delaysPath)\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\"))\n",
    "delays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1078b1b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "foo: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create temporary small table\n",
    "val foo = delays.filter(\n",
    " expr(\"\"\"origin == 'SEA' AND destination == 'SFO' AND \n",
    " date like '01010%' AND delay > 0\"\"\"))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad04c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332015e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77ef01fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66fe10c",
   "metadata": {},
   "source": [
    "**Unions**:  dos tablas con las mismas columnas, unimos filas al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb532ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bar: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Union two tables\n",
    "val bar = delays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc72c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM bar LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8ca3331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM bar WHERE origin = 'SEA' AND destination = 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b0cebb",
   "metadata": {},
   "source": [
    "**Joins**: a traves de la PK unimos combinamos columnas de diferentes DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4ec9ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(airports.as('air), $\"air.IATA\" === $\"origin\").select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()\n",
    "    \n",
    "spark.sql(\"\"\"SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination  FROM foo f JOIN airports_na a\n",
    "ON a.IATA = f.origin\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca2b2db",
   "metadata": {},
   "source": [
    "**Modifications**: adding new columns, dropping columns, renaming columns, pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d466341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0534e468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.expr\r\n",
       "foo2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 4 more fields]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Adding new columns\n",
    "import org.apache.spark.sql.functions.expr\n",
    "val foo2 = foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\"))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12a08614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo3: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Dropping columns\n",
    "val foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c584c909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "foo4: org.apache.spark.sql.DataFrame = [date: string, distance: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Renaming columns\n",
    "val foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11407d",
   "metadata": {},
   "source": [
    "*Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e97c1f",
   "metadata": {},
   "source": [
    "**Creamos nuestra base de datos para los ejemplos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd6c822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "923bfff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tripdelaysFilePath =\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv\"\n",
    "airportsnaFilePath =\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/airport-codes-na.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6275c9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain airports data set\n",
    "airportsna = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
    " .load(airportsnaFilePath))\n",
    "airportsna.createOrReplaceTempView(\"airports_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "857dbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain departure delays data set\n",
    "departureDelays = (spark.read\n",
    " .format(\"csv\")\n",
    " .options(header=\"true\")\n",
    " .load(tripdelaysFilePath))\n",
    "\n",
    "departureDelays = (departureDelays\n",
    " .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
    " .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
    "departureDelays.createOrReplaceTempView(\"departureDelays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aebf6bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary small table\n",
    "foo = (departureDelays\n",
    " .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and \n",
    " date like '01010%' and delay > 0\"\"\")))\n",
    "foo.createOrReplaceTempView(\"foo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7227e4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-------+----+\n",
      "|       City|State|Country|IATA|\n",
      "+-----------+-----+-------+----+\n",
      "| Abbotsford|   BC| Canada| YXX|\n",
      "|   Aberdeen|   SD|    USA| ABR|\n",
      "|    Abilene|   TX|    USA| ABI|\n",
      "|      Akron|   OH|    USA| CAK|\n",
      "|    Alamosa|   CO|    USA| ALS|\n",
      "|     Albany|   GA|    USA| ABY|\n",
      "|     Albany|   NY|    USA| ALB|\n",
      "|Albuquerque|   NM|    USA| ABQ|\n",
      "| Alexandria|   LA|    USA| AEX|\n",
      "|  Allentown|   PA|    USA| ABE|\n",
      "+-----------+-----+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM airports_na LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a74ce674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93aa2371",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM foo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458b02f6",
   "metadata": {},
   "source": [
    "**Unions**: dos tablas con las mismas columnas, unimos filas al final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b771d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union two tables\n",
    "bar = departureDelays.union(foo)\n",
    "bar.createOrReplaceTempView(\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dec15b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM bar LIMIT 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df57f5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the union (filtering for SEA and SFO in a specific time range)\n",
    "bar.filter(expr(\"\"\"origin == 'SEA' AND destination == 'SFO'\n",
    "AND date LIKE '01010%' AND delay > 0\"\"\")).show()\n",
    "\n",
    "spark.sql(\"\"\"SELECT * FROM bar WHERE origin = 'SEA' AND destination = 'SFO' AND date LIKE '01010%' AND delay > 0\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb77e8f6",
   "metadata": {},
   "source": [
    "**Joins**: a traves de la PK unimos combinamos columnas de diferentes DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "09dd1b91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|   City|State|    date|delay|distance|destination|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "|Seattle|   WA|01010710|   31|     590|        SFO|\n",
      "|Seattle|   WA|01010955|  104|     590|        SFO|\n",
      "|Seattle|   WA|01010730|    5|     590|        SFO|\n",
      "+-------+-----+--------+-----+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.join(airportsna, airportsna.IATA == foo.origin).select(\"City\", \"State\", \"date\", \"delay\", \"distance\", \"destination\").show()\n",
    "\n",
    "spark.sql(\"\"\"SELECT a.City, a.State, f.date, f.delay, f.distance, f.destination  FROM foo f JOIN airports_na a\n",
    "ON a.IATA = f.origin\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357bf32",
   "metadata": {},
   "source": [
    "**Windowing**: utiliza los valores de las filas de una ventana para devolver un conjunto de valores, en forma de otra fila. Con estas funciones es posible operar en un grupo de filas mientras se devuelve un unico valor para cada fila de entrada. (pag173)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dc9ac080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"DROP TABLE IF EXISTS departureDelaysWindow;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "309330e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE departureDelaysWindow AS\n",
    "SELECT origin, destination, SUM(delay) AS TotalDelays\n",
    "FROM departureDelays\n",
    "WHERE origin IN ('SEA', 'SFO', 'JFK')\n",
    "AND destination IN ('SEA', 'SFO', 'JFK', 'DEN', 'ORD', 'LAX', 'ATL')\n",
    "GROUP BY origin, destination;\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d79c58e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        ORD|       5608|\n",
      "|   SEA|        LAX|       9359|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   SFO|        ORD|      27412|\n",
      "|   JFK|        DEN|       4315|\n",
      "|   SFO|        DEN|      18688|\n",
      "|   SFO|        SEA|      17080|\n",
      "|   SEA|        SFO|      22293|\n",
      "|   JFK|        ATL|      12141|\n",
      "|   SFO|        ATL|       5091|\n",
      "+------+-----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * FROM departureDelaysWindow\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4ff3bd45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+\n",
      "|origin|destination|TotalDelays|\n",
      "+------+-----------+-----------+\n",
      "|   JFK|        LAX|      35755|\n",
      "|   JFK|        SFO|      35619|\n",
      "|   JFK|        ATL|      12141|\n",
      "+------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT origin, destination, SUM(TotalDelays) AS TotalDelays\n",
    "FROM departureDelaysWindow\n",
    "WHERE origin = 'JFK'\n",
    "GROUP BY origin, destination\n",
    "ORDER BY SUM(TotalDelays) DESC\n",
    "LIMIT 3\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9bb540f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------+----+\n",
      "|origin|destination|TotalDelays|rank|\n",
      "+------+-----------+-----------+----+\n",
      "|   SEA|        SFO|      22293|   1|\n",
      "|   SEA|        DEN|      13645|   2|\n",
      "|   SEA|        ORD|      10041|   3|\n",
      "|   SFO|        LAX|      40798|   1|\n",
      "|   SFO|        ORD|      27412|   2|\n",
      "|   SFO|        JFK|      24100|   3|\n",
      "|   JFK|        LAX|      35755|   1|\n",
      "|   JFK|        SFO|      35619|   2|\n",
      "|   JFK|        ATL|      12141|   3|\n",
      "+------+-----------+-----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT origin, destination, TotalDelays, rank FROM ( \n",
    " SELECT origin, destination, TotalDelays, dense_rank() \n",
    " OVER (PARTITION BY origin ORDER BY TotalDelays DESC) as rank \n",
    " FROM departureDelaysWindow) t WHERE rank <= 3\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45ed4b",
   "metadata": {},
   "source": [
    "**Modifications**: adding new columns, dropping columns, renaming columns, pivoting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b6d2f47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01010710|   31|     590|   SEA|        SFO|\n",
      "|01010955|  104|     590|   SEA|        SFO|\n",
      "|01010730|    5|     590|   SEA|        SFO|\n",
      "+--------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "foo.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "db4f2720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+-------+\n",
      "|    date|delay|distance|origin|destination| status|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "|01010710|   31|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|  104|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|    5|     590|   SEA|        SFO|On-time|\n",
      "+--------+-----+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Adding new columns\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "foo2 = (foo.withColumn(\"status\", expr(\"CASE WHEN delay <= 10 THEN 'On-time' ELSE 'Delayed' END\")))\n",
    "foo2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "acebf2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------+\n",
      "|    date|distance|origin|destination| status|\n",
      "+--------+--------+------+-----------+-------+\n",
      "|01010710|     590|   SEA|        SFO|Delayed|\n",
      "|01010955|     590|   SEA|        SFO|Delayed|\n",
      "|01010730|     590|   SEA|        SFO|On-time|\n",
      "+--------+--------+------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Dropping columns\n",
    "foo3 = foo2.drop(\"delay\")\n",
    "foo3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "983ebcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+-----------+-------------+\n",
      "|    date|distance|origin|destination|flight_status|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "|01010710|     590|   SEA|        SFO|      Delayed|\n",
      "|01010955|     590|   SEA|        SFO|      Delayed|\n",
      "|01010730|     590|   SEA|        SFO|      On-time|\n",
      "+--------+--------+------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Renaming columns\n",
    "foo4 = foo3.withColumnRenamed(\"status\", \"flight_status\")\n",
    "foo4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd4c6d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+\n",
      "|destination|month|delay|\n",
      "+-----------+-----+-----+\n",
      "|        ORD|    1|   92|\n",
      "|        JFK|    1|   -7|\n",
      "|        DFW|    1|   -5|\n",
      "|        MIA|    1|   -3|\n",
      "|        DFW|    1|   -3|\n",
      "|        DFW|    1|    1|\n",
      "|        ORD|    1|  -10|\n",
      "|        DFW|    1|   -6|\n",
      "|        DFW|    1|   -2|\n",
      "|        ORD|    1|   -3|\n",
      "|        ORD|    1|    0|\n",
      "|        DFW|    1|   23|\n",
      "|        DFW|    1|   36|\n",
      "|        ORD|    1|  298|\n",
      "|        JFK|    1|    4|\n",
      "|        DFW|    1|    0|\n",
      "|        MIA|    1|    2|\n",
      "|        DFW|    1|    0|\n",
      "|        DFW|    1|    0|\n",
      "|        ORD|    1|   83|\n",
      "+-----------+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|destination|JAN_AvgDelay|JAN_MaxDelay|FEB_AvgDelay|FEB_MaxDelay|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "|        ABQ|       19.86|         316|       11.42|          69|\n",
      "|        ANC|        4.44|         149|        7.90|         141|\n",
      "|        ATL|       11.98|         397|        7.73|         145|\n",
      "|        AUS|        3.48|          50|       -0.21|          18|\n",
      "|        BOS|        7.84|         110|       14.58|         152|\n",
      "|        BUR|       -2.03|          56|       -1.89|          78|\n",
      "|        CLE|       16.00|          27|        null|        null|\n",
      "|        CLT|        2.53|          41|       12.96|         228|\n",
      "|        COS|        5.32|          82|       12.18|         203|\n",
      "|        CVG|       -0.50|           4|        null|        null|\n",
      "|        DCA|       -1.15|          50|        0.07|          34|\n",
      "|        DEN|       13.13|         425|       12.95|         625|\n",
      "|        DFW|        7.95|         247|       12.57|         356|\n",
      "|        DTW|        9.18|         107|        3.47|          77|\n",
      "|        EWR|        9.63|         236|        5.20|         212|\n",
      "|        FAI|        1.84|         160|        4.21|          60|\n",
      "|        FAT|        1.36|         119|        5.22|         232|\n",
      "|        FLL|        2.94|          54|        3.50|          40|\n",
      "|        GEG|        2.28|          63|        2.87|          60|\n",
      "|        HDN|       -0.44|          27|       -6.50|           0|\n",
      "+-----------+------------+------------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Pivoting\n",
    "spark.sql(\"\"\"SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay FROM departureDelays \n",
    "WHERE origin = 'SEA'\"\"\").show()\n",
    "\n",
    "\n",
    "spark.sql(\"\"\"SELECT * \n",
    "FROM (SELECT destination, CAST(SUBSTRING(date, 0, 2) AS int) AS month, delay FROM departureDelays WHERE origin = 'SEA')\n",
    "PIVOT (CAST(AVG(delay) AS DECIMAL(4, 2)) AS AvgDelay, MAX(delay) AS MaxDelay FOR month IN (1 JAN, 2 FEB))\n",
    "ORDER BY destination\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
