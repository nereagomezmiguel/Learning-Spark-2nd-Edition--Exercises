{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00b30d3b",
   "metadata": {},
   "source": [
    "**RDDs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a495a70b",
   "metadata": {},
   "source": [
    "Las RDDs son las abstracciones más básicas de Spark. Tienen tres caracteristicas asociadas:  \n",
    "\t- Dependencias: dicen a Spark como esta construido un RDD y que inputs requiere.  \n",
    "\t- Particiones (con algo de información local): permiten a Spark dividir el trabajo.  \n",
    "\t- Funciones de computación: Partition =>Iterator[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb51f47",
   "metadata": {},
   "source": [
    "**Ejemplo 1: low-level RDD: Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306657d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos un RDD con tuplas (name,age)\n",
    "dataRDD = sc.parallelize([(\"Brooke\", 20), (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270ad1b5",
   "metadata": {},
   "source": [
    "Con el siguiente RDD vamos a utilizar la función reduceByKey para agrupar las tuplas con la misma clave y vamos a calcular la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58522c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "agesRDD = dataRDD.map(lambda x: (x[0], (x[1], 1))).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])).map(lambda x: (x[0], x[1][0]/x[1][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f77a496d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Brooke', 22.5), ('Denny', 31.0), ('TD', 35.0), ('Jules', 30.0)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agesRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca2e155",
   "metadata": {},
   "source": [
    "Este código se ejecuta, pero a la hora de realizar una accion sobre él obtendremos como resultado un error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e17dc",
   "metadata": {},
   "source": [
    "**Ejemplo 2: high-level DSL operators and the DataFrame: Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46ba084",
   "metadata": {},
   "source": [
    "Importamos las librerias de Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7d96d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "import sys\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31de72a3",
   "metadata": {},
   "source": [
    "Creamos la SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd7b0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"AuthorsAges\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c2e575",
   "metadata": {},
   "source": [
    "Cremaos un DataFrame con los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1d577e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDF = spark.createDataFrame([(\"Broke\",20), (\"Denny\",31), (\"Jules\",30), (\"TD\",35),\n",
    "                         (\"Broke\",25)], [\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e67df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Broke| 20|\n",
      "|Denny| 31|\n",
      "|Jules| 30|\n",
      "|   TD| 35|\n",
      "|Broke| 25|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f61390",
   "metadata": {},
   "source": [
    "Agrupamos los datos por nombre y calculamos la media de la edad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58543d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_df = (dataDF\n",
    "          .groupBy(\"name\")\n",
    "          .agg(avg(\"age\").alias(\"mean\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d219ee4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+\n",
      "|name |mean|\n",
      "+-----+----+\n",
      "|Jules|30.0|\n",
      "|Broke|22.5|\n",
      "|TD   |35.0|\n",
      "|Denny|31.0|\n",
      "+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_df.show(n=5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02ad82",
   "metadata": {},
   "source": [
    "**Ejemplo 2: high-level DSL operators and the DataFrame: Scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ab7b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002836.bosonit.local:4041\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1622799803219)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Importamos las librerias necesarias\n",
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d47c5929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@628b81be\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creamos un DataFrame a través de SparkSession\n",
    "val spark = SparkSession\n",
    "            .builder\n",
    "            .appName(\"AuthorAges\")\n",
    "            .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6b659bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataDF: org.apache.spark.sql.DataFrame = [name: string, age: int]\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creamos un DataFrame con nombres y edades\n",
    "val dataDF = spark.createDataFrame(Seq((\"Brooke\", 20), (\"Brooke\", 25),\n",
    " (\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35))).toDF(\"name\", \"age\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b843d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "avgDF: org.apache.spark.sql.DataFrame = [name: string, avg(age): double]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Agrupamos por nombre y calculamos la media de las edades agruupadas\n",
    "val avgDF = dataDF.groupBy(\"name\").agg(avg(\"age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0782abd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|  name|avg(age)|\n",
      "+------+--------+\n",
      "|Brooke|    22.5|\n",
      "| Jules|    30.0|\n",
      "|    TD|    35.0|\n",
      "| Denny|    31.0|\n",
      "+------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Mostramos el resultado final\n",
    "avgDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8571a80a",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be1ed7a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9b9ae",
   "metadata": {},
   "source": [
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8695ef4c",
   "metadata": {},
   "source": [
    "## The DataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21393427",
   "metadata": {},
   "source": [
    "Los Spark DataFrames son como tablas distribuidas en memoria con nombre de columnas y esquemas, donde se especifica el tipo de cada columna."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a437446",
   "metadata": {},
   "source": [
    "**Ejemplo Basic Data Types: Scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6649b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80bf1d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nameTypes: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val nameTypes = StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f708047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstName: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstName = nameTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ca9aa93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastName: org.apache.spark.sql.types.StringType.type = StringType\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lastName = nameTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8aa73e",
   "metadata": {},
   "source": [
    "**Ejemplo two way to define a schema: Scala**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c468f79c",
   "metadata": {},
   "source": [
    "*Spark DataFrame API*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60ba9f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0a1ab4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(author,StringType,false), StructField(title,StringType,false), StructField(pages,IntegerType,false))\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(StructField(\"author\", StringType, false),\n",
    " StructField(\"title\", StringType, false),\n",
    " StructField(\"pages\", IntegerType, false)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3f32fd",
   "metadata": {},
   "source": [
    "*DDL: Data Definition Lenguage*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1d24012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: String = author STRING, title STRING, pages INT\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b99bce",
   "metadata": {},
   "source": [
    "*Ejemplo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c6f944e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7997d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@628b81be\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_7\")\n",
    " .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64acd113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false), StructField(First,StringType,false), StructField(Last,StringType,false), StructField(Url,StringType,false), StructField(Published,StringType,false), StructField(Hits,IntegerType,false), StructField(Campaigns,ArrayType(StringType,true),false))\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    "     StructField(\"First\", StringType, false),\n",
    "     StructField(\"Last\", StringType, false),\n",
    "     StructField(\"Url\", StringType, false),\n",
    "     StructField(\"Published\", StringType, false),\n",
    "     StructField(\"Hits\", IntegerType, false),\n",
    "     StructField(\"Campaigns\", ArrayType(StringType), false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02f35bf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "jsonFile: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/blogs.json\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val jsonFile = \"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/blogs.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f37fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blogsDF: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val blogsDF = spark.read.schema(schema).json(jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "618b1ec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|Id |First    |Last   |Url              |Published|Hits |Campaigns                   |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "|1  |Jules    |Damji  |https://tinyurl.1|1/4/2016 |4535 |[twitter, LinkedIn]         |\n",
      "|2  |Brooke   |Wenig  |https://tinyurl.2|5/5/2018 |8908 |[twitter, LinkedIn]         |\n",
      "|3  |Denny    |Lee    |https://tinyurl.3|6/7/2019 |7659 |[web, twitter, FB, LinkedIn]|\n",
      "|4  |Tathagata|Das    |https://tinyurl.4|5/12/2018|10568|[twitter, FB]               |\n",
      "|5  |Matei    |Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB, LinkedIn]|\n",
      "|6  |Reynold  |Xin    |https://tinyurl.6|3/2/2015 |25568|[twitter, LinkedIn]         |\n",
      "+---+---------+-------+-----------------+---------+-----+----------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8eaeb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "()\n",
      "StructType(StructField(Id,IntegerType,true), StructField(First,StringType,true), StructField(Last,StringType,true), StructField(Url,StringType,true), StructField(Published,StringType,true), StructField(Hits,IntegerType,true), StructField(Campaigns,ArrayType(StringType,true),true))\n"
     ]
    }
   ],
   "source": [
    "println(blogsDF.printSchema)\n",
    "println(blogsDF.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4ed82a",
   "metadata": {},
   "source": [
    "**Ejemplo two way to define a schema: Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0d5dc",
   "metadata": {},
   "source": [
    "*Spark DataFrame API*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4310190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0de1ac4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    " StructField(\"title\", StringType(), False),\n",
    " StructField(\"pages\", IntegerType(), False)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a929cf1",
   "metadata": {},
   "source": [
    "*DDL*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b65214b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d84814c",
   "metadata": {},
   "source": [
    "*Ejemplo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dde15e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29aa7724",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36eb2988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    [1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\", \"LinkedIn\"]],\n",
    "    [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "    [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "    [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "    [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f279444",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    " .builder\n",
    " .appName(\"Example-3_6\")\n",
    " .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8819bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogs_df = spark.createDataFrame(data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0674d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b11044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "|     15318|\n",
      "|     21136|\n",
      "|     81156|\n",
      "|     51136|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Para multiplicar una columna\n",
    "(blogs_df\n",
    " .select(F.col(\"Hits\")*2)\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "456f4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "309da359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Id,IntegerType,true),StructField(First,StringType,true),StructField(Last,StringType,true),StructField(Url,StringType,true),StructField(Published,StringType,true),StructField(Hits,IntegerType,true),StructField(Campaigns,ArrayType(StringType,true),true)))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Para mirar el esquema en cualquier parte del código\n",
    "blogs_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d47f0",
   "metadata": {},
   "source": [
    "**Ejemplo columnas: Scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "325d138b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd0a282e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: Array[String] = Array(Id, First, Last, Url, Published, Hits, Campaigns)\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogsDF.columns //muestra el nombre de las columnas de nuestro DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b305697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: org.apache.spark.sql.Column = Id\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogsDF.col(\"Id\") //para mostrar que existe una columna con ese nombre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b586473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(expr(\"Hits * 2\")).show(2) //Multiplica por 2 la columna Hits y muestra las dos primeras filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fce78001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|(Hits * 2)|\n",
      "+----------+\n",
      "|      9070|\n",
      "|     17816|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(col(\"Hits\") * 2).show(2) //igual que el anterior a traves de la funcion col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67857f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|Big Hitters|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|      false|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|      false|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|      false|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|       true|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|       true|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|       true|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Creamos una nueva columna a partir de una ya existente teniendo que cumplir la condicion dada\n",
    "blogsDF.withColumn(\"Big Hitters\", (expr(\"Hits > 10000\"))).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e190254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|    AuthorsId|\n",
      "+-------------+\n",
      "|  JulesDamji1|\n",
      "| BrookeWenig2|\n",
      "|    DennyLee3|\n",
      "|TathagataDas4|\n",
      "+-------------+\n",
      "only showing top 4 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Concatenar tres columnas existentes para crear una nueva\n",
    "blogsDF\n",
    " .withColumn(\"AuthorsId\", (concat(expr(\"First\"), expr(\"Last\"), expr(\"Id\"))))\n",
    " .select(col(\"AuthorsId\"))\n",
    " .show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af768d7",
   "metadata": {},
   "source": [
    "Tres métodos distintos para seleccionar una columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d029b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(expr(\"Hits\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22087a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(col(\"Hits\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f3a3ee03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Hits|\n",
      "+----+\n",
      "|4535|\n",
      "|8908|\n",
      "+----+\n",
      "only showing top 2 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.select(\"Hits\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c8e415",
   "metadata": {},
   "source": [
    "Dos metodos para ordenar el DataFrame a través de una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ff073c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.sort(col(\"Id\").desc).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "543972d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "blogsDF.sort($\"Id\".desc).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060ea7d2",
   "metadata": {},
   "source": [
    "**Ejemplo Filas: Scala**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5f6294c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf4bcb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "blogRow: org.apache.spark.sql.Row = [6,Reynold,Xin,https://tinyurl.6,255568,3/2/2015,[Ljava.lang.String;@712d3150]\r\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val blogRow = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\", Array(\"twitter\", \"LinkedIn\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f3913bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Any = Reynold\r\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Para acceder al segundo elemento de la linea\n",
    "blogRow(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd38369d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|       Author|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "rows: Seq[(String, String)] = List((Matei Zaharia,CA), (Reynold Xin,CA))\r\n",
       "authorsDF: org.apache.spark.sql.DataFrame = [Author: string, State: string]\r\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Crear DataFrames a traves de lineas\n",
    "val rows = Seq((\"Matei Zaharia\", \"CA\"), (\"Reynold Xin\", \"CA\"))\n",
    "val authorsDF = rows.toDF(\"Author\", \"State\")\n",
    "authorsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566c6afb",
   "metadata": {},
   "source": [
    "**Ejemplo Filas: Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30bc5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "781f8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_row = Row(6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", 255568, \"3/2/2015\",\n",
    " [\"twitter\", \"LinkedIn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fb5b1ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reynold'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Para acceder a una linea\n",
    "blog_row[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1fefd6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|      Authors|State|\n",
      "+-------------+-----+\n",
      "|Matei Zaharia|   CA|\n",
      "|  Reynold Xin|   CA|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Crear un DataFrame a través de lineas\n",
    "rows = [Row(\"Matei Zaharia\", \"CA\"), Row(\"Reynold Xin\", \"CA\")]\n",
    "authors_df = spark.createDataFrame(rows, [\"Authors\", \"State\"])\n",
    "authors_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86712156",
   "metadata": {},
   "source": [
    "**spark.read.csv() --> lee un CSV y devuelve un DataFrame con las filas y nombres de columnas definidos por un esquema anterior.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23467ab4",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74a7637",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785bf19c",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597c3aa0",
   "metadata": {},
   "source": [
    "### Ejemplo fire-calls: Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96dd3b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import sys \n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "43206871",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creamos el esquema del DF\n",
    "fire_schema = StructType([StructField('CallNumber', IntegerType(), True),\n",
    "  StructField('UnitID', StringType(), True),\n",
    "  StructField('IncidentNumber', IntegerType(), True),\n",
    "  StructField('CallType', StringType(), True), \n",
    "  StructField('CallDate', StringType(), True), \n",
    "  StructField('WatchDate', StringType(), True),\n",
    "  StructField('CallFinalDisposition', StringType(), True),\n",
    "  StructField('AvailableDtTm', StringType(), True),\n",
    "  StructField('Address', StringType(), True), \n",
    "  StructField('City', StringType(), True), \n",
    "  StructField('Zipcode', IntegerType(), True), \n",
    "  StructField('Battalion', StringType(), True), \n",
    "  StructField('StationArea', StringType(), True), \n",
    "  StructField('Box', StringType(), True), \n",
    "  StructField('OriginalPriority', StringType(), True), \n",
    "  StructField('Priority', StringType(), True), \n",
    "  StructField('FinalPriority', IntegerType(), True), \n",
    "  StructField('ALSUnit', BooleanType(), True), \n",
    "  StructField('CallTypeGroup', StringType(), True),\n",
    "  StructField('NumAlarms', IntegerType(), True),\n",
    "  StructField('UnitType', StringType(), True),\n",
    "  StructField('UnitSequenceInCallDispatch', IntegerType(), True),\n",
    "  StructField('FirePreventionDistrict', StringType(), True),\n",
    "  StructField('SupervisorDistrict', StringType(), True),\n",
    "  StructField('Neighborhood', StringType(), True),\n",
    "  StructField('Location', StringType(), True),\n",
    "  StructField('RowID', StringType(), True),\n",
    "  StructField('Delay', FloatType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "72dcf55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Leemos los datos y creamos el DF\n",
    "sf_fire_file = \"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/sf-fire-calls.csv\"\n",
    "fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e0529",
   "metadata": {},
   "source": [
    "Guardar los datos en formato Parquet: conserva el esquema de los datos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6438c2f7",
   "metadata": {},
   "source": [
    "fire_df.write.format(\"parquet\").save(\"parquet_path\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bca6fdf",
   "metadata": {},
   "source": [
    "Guardar los datos como tabla de SQL:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f4cd99d",
   "metadata": {},
   "source": [
    "fire_df.write.format(\"parquet\").saveAsTable(\"parquet_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd6ecd3",
   "metadata": {},
   "source": [
    "**Ejemplo Projections and filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e116ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Seleccionamos tres columnas donde del tipo de llamada no sea un accidente medico\n",
    "few_fire_df = (fire_df\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where(fire_df.CallType != \"Medical Incident\"))\n",
    "few_fire_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "588410f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Contamos cuantos valores hay en la columna CallType que no sean nulos\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(fire_df.CallType.isNotNull())\n",
    " .agg(F.countDistinct(\"CallType\").alias(\"DistinctCallTypes\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d9476e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+\n",
      "|CallType                           |\n",
      "+-----------------------------------+\n",
      "|Elevator / Escalator Rescue        |\n",
      "|Marine Fire                        |\n",
      "|Aircraft Emergency                 |\n",
      "|Confined Space / Structure Collapse|\n",
      "|Administrative                     |\n",
      "|Alarms                             |\n",
      "|Odor (Strange / Unknown)           |\n",
      "|Citizen Assist / Service Call      |\n",
      "|HazMat                             |\n",
      "|Watercraft in Distress             |\n",
      "+-----------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Mostramos los diferentes valores que toma la columna CallType\n",
    "(fire_df\n",
    " .select(\"CallType\")\n",
    " .where(fire_df.CallType.isNotNull())\n",
    " .distinct()\n",
    " .show(10, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b349e878",
   "metadata": {},
   "source": [
    "**Example renaming, adding and dropping columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a67e29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Creamos un nuevo dataframe a partir de las columnas seleccionadas\n",
    "new_fire_df = fire_df.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "(new_fire_df\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where(new_fire_df.ResponseDelayedinMins > 5)\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "289ab20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creamos un nuevo DataFrame con tres columnas nuevas en formato fecha a partir de la funcion to_tiemstamp\n",
    "fire_ts_df = (new_fire_df\n",
    " .withColumn(\"IncidentDate\", F.to_timestamp(new_fire_df.CallDate, \"MM/dd/yyyy\")).drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", F.to_timestamp(new_fire_df.WatchDate, \"MM/dd/yyyy\")).drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", F.to_timestamp(new_fire_df.AvailableDtTm, \"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeb8912f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Mostramos las cinco primeras lineas de las tres nuevas variables\n",
    "(fire_ts_df\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0cbccc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##De la columna IncidentDate seleccionamos los años que puede tomar esa columna como valor\n",
    "(fire_ts_df\n",
    " .select(F.year('IncidentDate'))\n",
    " .distinct()\n",
    " .orderBy(F.year('IncidentDate'))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d203ef",
   "metadata": {},
   "source": [
    "**Example aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35c6749e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Seleccionamos la columna calltype donde el valor no sea nulo, lo agrupamos por valor y contamos cuantas veces aparece\n",
    "##dicho valor, ordenamos el resultado por el numero de veces que aparece\n",
    "(fire_ts_df\n",
    " .select(\"CallType\")\n",
    " .where(fire_ts_df.CallType.isNotNull())\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=False)\n",
    " .show(n=10, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779c99d9",
   "metadata": {},
   "source": [
    "**Example other common DF operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1d91e7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##Calculamos la suma, media, minimo y maximo de algunas columnas y mostramos el resultado\n",
    "(fire_ts_df\n",
    " .select(F.sum(\"NumAlarms\"), F.avg(\"ResponseDelayedinMins\"),F.min(\"ResponseDelayedinMins\"), F.max(\"ResponseDelayedinMins\"))\n",
    " .show())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5051be12",
   "metadata": {},
   "source": [
    "Otras funciones son stat(), describe(), correlation(), covariance(), sampleBy(), approxQuantile(), frequenItems(), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45adf6ce",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cf99db",
   "metadata": {},
   "source": [
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906b0fb8",
   "metadata": {},
   "source": [
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde701fd",
   "metadata": {},
   "source": [
    "### Ejemplo fire-calls: Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "66714f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.avg\r\n",
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Importamos las librerias necesarias\n",
    "import org.apache.spark.sql.functions.avg\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24f6d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleDF: org.apache.spark.sql.DataFrame = [CallNumber: string, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Para que Spark establezca un esquema de forma automatica\n",
    "val sampleDF = spark\n",
    " .read\n",
    " .option(\"samplingRatio\", 0.001)\n",
    " .option(\"header\", true)\n",
    " .csv(\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/sf-fire-calls.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9a9b9dfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireSchema: org.apache.spark.sql.types.StructType = StructType(StructField(CallNumber,IntegerType,true), StructField(UnitID,StringType,true), StructField(IncidentNumber,IntegerType,true), StructField(CallType,StringType,true), StructField(CallDate,StringType,true), StructField(WatchDate,StringType,true), StructField(CallFinalDisposition,StringType,true), StructField(AvailableDtTm,StringType,true), StructField(Address,StringType,true), StructField(City,StringType,true), StructField(Zipcode,IntegerType,true), StructField(Battalion,StringType,true), StructField(StationArea,StringType,true), StructField(Box,StringType,true), StructField(OriginalPriority,StringType,true), StructField(Priority,StringType,true), StructField(FinalPriority,StringType,true), StructField(ALSUnit,BooleanType,true),...\r\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creamos la variable con el esquema que van a tener nuestros datos\n",
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "                                  StructField(\"UnitID\", StringType, true),\n",
    "                                  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "                                  StructField(\"CallType\", StringType, true),\n",
    "                                  StructField(\"CallDate\", StringType, true),\n",
    "                                  StructField(\"WatchDate\", StringType, true),\n",
    "                                  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "                                  StructField(\"AvailableDtTm\", StringType, true),\n",
    "                                  StructField(\"Address\", StringType, true),\n",
    "                                  StructField(\"City\", StringType, true),\n",
    "                                  StructField(\"Zipcode\", IntegerType, true),\n",
    "                                  StructField(\"Battalion\", StringType, true),\n",
    "                                  StructField(\"StationArea\", StringType, true),\n",
    "                                  StructField(\"Box\", StringType, true),\n",
    "                                  StructField(\"OriginalPriority\", StringType, true),\n",
    "                                  StructField(\"Priority\", StringType, true),\n",
    "                                  StructField(\"FinalPriority\", StringType, true),\n",
    "                                  StructField(\"ALSUnit\", BooleanType, true),\n",
    "                                  StructField(\"CallTypeGroup\", StringType, true),\n",
    "                                  StructField(\"NumAlarms\", IntegerType, true),\n",
    "                                  StructField(\"UnitType\", StringType, true),\n",
    "                                  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "                                  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "                                  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "                                  StructField(\"Neighborhood\", StringType, true),       \n",
    "                                  StructField(\"Location\", StringType, true),\n",
    "                                  StructField(\"RowID\", StringType, true),\n",
    "                                  StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87e658bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sfFireFile: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/sf-fire-calls.csv\r\n",
       "fireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Leemos los datos y creamos el dataframe\n",
    "val sfFireFile=\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/sf-fire-calls.csv\"\n",
    "val fireDF = spark.read.schema(fireSchema)\n",
    " .option(\"header\", \"true\")\n",
    " .csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ef355",
   "metadata": {},
   "source": [
    "Guardar los datos en formato Parquet: conserva el esquema de los datos"
   ]
  },
  {
   "cell_type": "raw",
   "id": "890a7920",
   "metadata": {},
   "source": [
    "fireDF.write.format(\"csv\").save(\"parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64145bb5",
   "metadata": {},
   "source": [
    "Guardar los datos como tabla de SQL:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc85ad2f",
   "metadata": {},
   "source": [
    "fireDF.write.format(\"parquet\").saveAsTable(\"parquetTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5716c501",
   "metadata": {},
   "source": [
    "**Ejemplo Projections and filters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "633d8764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------------+--------------+\n",
      "|IncidentNumber|AvailableDtTm         |CallType      |\n",
      "+--------------+----------------------+--------------+\n",
      "|2003235       |01/11/2002 01:51:44 AM|Structure Fire|\n",
      "|2003250       |01/11/2002 04:16:46 AM|Vehicle Fire  |\n",
      "|2003259       |01/11/2002 06:01:58 AM|Alarms        |\n",
      "|2003279       |01/11/2002 08:03:26 AM|Structure Fire|\n",
      "|2003301       |01/11/2002 09:46:44 AM|Alarms        |\n",
      "+--------------+----------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fewFireDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [IncidentNumber: int, AvailableDtTm: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Seleccionamos las filas que no sean igual a medical incident en la columna de calltype\n",
    "val fewFireDF = fireDF\n",
    " .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\")\n",
    " .where($\"CallType\" =!= \"Medical Incident\") \n",
    "\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0c5e810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DistinctCallTypes|\n",
      "+-----------------+\n",
      "|               30|\n",
      "+-----------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//De la columna calltype seleccionamos los valores que no son nulos y contamos cuantos diferentes hay.\n",
    "fireDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .agg(countDistinct('CallType) as 'DistinctCallTypes)\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef58edde",
   "metadata": {},
   "source": [
    "//De la columna calltype seleccionamos los valores que no son nulos y mostramos los distintos valores que puede tomar\n",
    "fireDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull())\n",
    " .distinct()\n",
    " .show(10, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91f9cf",
   "metadata": {},
   "source": [
    "**Ejercicio renaming, adding and dropping columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7a5e526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|ResponseDelayedinMins|\n",
      "+---------------------+\n",
      "|5.35                 |\n",
      "|6.25                 |\n",
      "|5.2                  |\n",
      "|5.6                  |\n",
      "|7.25                 |\n",
      "+---------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newFireDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Tenemos que crear un nuevo dataFrame cuando utilizamos la funcion withcolumrenamed para que las originales no se pierdan\n",
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "newFireDF\n",
    " .select(\"ResponseDelayedinMins\")\n",
    " .where($\"ResponseDelayedinMins\" > 5)\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "01ebab39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fireTsDF: org.apache.spark.sql.DataFrame = [CallNumber: int, UnitID: string ... 26 more fields]\r\n"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//creamos tres nuevas columnas a partir de las existentes en formato fecha\n",
    "val fireTsDF = newFireDF\n",
    " .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"CallDate\")\n",
    " .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\"))\n",
    " .drop(\"WatchDate\")\n",
    " .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"),\n",
    " \"MM/dd/yyyy hh:mm:ss a\"))\n",
    " .drop(\"AvailableDtTm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10a30dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+-------------------+\n",
      "|IncidentDate       |OnWatchDate        |AvailableDtTS      |\n",
      "+-------------------+-------------------+-------------------+\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 01:51:44|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 03:01:18|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 02:39:50|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 04:16:46|\n",
      "|2002-01-11 00:00:00|2002-01-10 00:00:00|2002-01-11 06:01:58|\n",
      "+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Mostramos las cinco primeras lineas de nuevas columnas\n",
    "fireTsDF\n",
    " .select(\"IncidentDate\", \"OnWatchDate\", \"AvailableDtTS\")\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c8c3cf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|year(IncidentDate)|\n",
      "+------------------+\n",
      "|              2000|\n",
      "|              2001|\n",
      "|              2002|\n",
      "|              2003|\n",
      "|              2004|\n",
      "|              2005|\n",
      "|              2006|\n",
      "|              2007|\n",
      "|              2008|\n",
      "|              2009|\n",
      "|              2010|\n",
      "|              2011|\n",
      "|              2012|\n",
      "|              2013|\n",
      "|              2014|\n",
      "|              2015|\n",
      "|              2016|\n",
      "|              2017|\n",
      "|              2018|\n",
      "+------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Mostramos los diferentes años que toma como valor la variable incidentDate\n",
    "fireTsDF\n",
    " .select(year($\"IncidentDate\"))\n",
    " .distinct()\n",
    " .orderBy(year($\"IncidentDate\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a6dd0",
   "metadata": {},
   "source": [
    "**Example aggregations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c1f6bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+------+\n",
      "|CallType                       |count |\n",
      "+-------------------------------+------+\n",
      "|Medical Incident               |113794|\n",
      "|Structure Fire                 |23319 |\n",
      "|Alarms                         |19406 |\n",
      "|Traffic Collision              |7013  |\n",
      "|Citizen Assist / Service Call  |2524  |\n",
      "|Other                          |2166  |\n",
      "|Outside Fire                   |2094  |\n",
      "|Vehicle Fire                   |854   |\n",
      "|Gas Leak (Natural and LP Gases)|764   |\n",
      "|Water Rescue                   |755   |\n",
      "+-------------------------------+------+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Mostramos los diferentes valores que toma la variable CallType\n",
    "fireTsDF\n",
    " .select(\"CallType\")\n",
    " .where(col(\"CallType\").isNotNull)\n",
    " .groupBy(\"CallType\")\n",
    " .count()\n",
    " .orderBy(desc(\"count\"))\n",
    " .show(10, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fe31937b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|sum(NumAlarms)|avg(ResponseDelayedinMins)|min(ResponseDelayedinMins)|max(ResponseDelayedinMins)|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "|        176170|         3.892364154521585|               0.016666668|                   1844.55|\n",
      "+--------------+--------------------------+--------------------------+--------------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "//Calculamos el sumatorio, minimo, media y maximo de ciertas variables\n",
    "fireTsDF\n",
    " .select(sum(\"NumAlarms\"), avg(\"ResponseDelayedinMins\"),min(\"ResponseDelayedinMins\"), max(\"ResponseDelayedinMins\"))\n",
    " .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35dcd95",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc46794",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363bca9",
   "metadata": {},
   "source": [
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7d607",
   "metadata": {},
   "source": [
    "## The Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c14379",
   "metadata": {},
   "source": [
    "La API de los Dataset en Spark tiene una interfaz similar a la de los DataFrames. Tienen dos caracteristicas:   \n",
    "    - Typed: Dataset [T], In Scala&Java   \n",
    "    - Untyped: DataFrame=Dataset[Row], Alias in Scala    \n",
    "    \n",
    "Datasets: Java y Scala   \n",
    "DataFrames: R y Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a164524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "row: org.apache.spark.sql.Row = [350,true,Learning Spark 2E,null]\r\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Scala\n",
    "import org.apache.spark.sql.Row\n",
    "val row = Row(350, true, \"Learning Spark 2E\", null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dfc71d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res23: Int = 350\r\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getInt(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f86ff275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res24: Boolean = true\r\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row.getBoolean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930df1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Python\n",
    "from pyspark.sql import Row\n",
    "row = Row(350, True, \"Learning Spark 2E\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bbd528a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322a31c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14304a4",
   "metadata": {},
   "source": [
    "        si usas toPandas() obtienes resultados mas claros que con show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d292963",
   "metadata": {},
   "source": [
    "**Scala**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353db96c",
   "metadata": {},
   "source": [
    "**Creating Datasets:** para poder crear un Dataset tienes que conocer previamente el esquema de los datos, es decir, el tipo de datos que va a contener."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a18e1407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceIoTData\r\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class DeviceIoTData (battery_level: Long, c02_level: Long,\n",
    "cca2: String, cca3: String, cn: String, device_id: Long,\n",
    "device_name: String, humidity: Long, ip: String, latitude: Double,\n",
    "lcd: String, longitude: Double, scale:String, temp: Long,\n",
    "timestamp: Long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30a9e820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ds: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ds = spark.read\n",
    ".json(\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/iot_devices.json\")\n",
    ".as[DeviceIoTData]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6566b330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|battery_level|c02_level|cca2|cca3|cn           |device_id|device_name          |humidity|ip           |latitude|lcd   |longitude|scale  |temp|timestamp    |\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "|8            |868      |US  |USA |United States|1        |meter-gauge-1xbYRYcj |51      |68.161.225.1 |38.0    |green |-97.0    |Celsius|34  |1458444054093|\n",
      "|7            |1473     |NO  |NOR |Norway       |2        |sensor-pad-2n2Pea    |70      |213.161.254.1|62.47   |red   |6.15     |Celsius|11  |1458444054119|\n",
      "|2            |1556     |IT  |ITA |Italy        |3        |device-mac-36TWSKiT  |44      |88.36.5.1    |42.83   |red   |12.83    |Celsius|19  |1458444054120|\n",
      "|6            |1080     |US  |USA |United States|4        |sensor-pad-4mzWkz    |32      |66.39.173.154|44.06   |yellow|-121.32  |Celsius|28  |1458444054121|\n",
      "|4            |931      |PH  |PHL |Philippines  |5        |therm-stick-5gimpUrBB|62      |203.82.41.9  |14.58   |green |120.97   |Celsius|25  |1458444054122|\n",
      "+-------------+---------+----+----+-------------+---------+---------------------+--------+-------------+--------+------+---------+-------+----+-------------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "ds.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bbf3e7",
   "metadata": {},
   "source": [
    "**Operaciones con Datasets:** igual que con DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "279c6842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterTempDS: org.apache.spark.sql.Dataset[DeviceIoTData] = [battery_level: bigint, c02_level: bigint ... 13 more fields]\r\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70}})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ab4ed7b0",
   "metadata": {},
   "source": [
    "filterTempDS.show(5,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f29a7b",
   "metadata": {},
   "source": [
    "*Otro ejemplo*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8b80da5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class DeviceTempByCountry\r\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creamos el esquema\n",
    "case class DeviceTempByCountry(temp: Long, device_name: String, device_id: Long,\n",
    " cca3: String)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57da9c02",
   "metadata": {},
   "source": [
    "//Creamos el Dataset\n",
    "val dsTemp = ds\n",
    " .filter(d => {d.temp > 25})\n",
    " .map(d => (d.temp, d.device_name, d.device_id, d.cca3))\n",
    " .toDF(\"temp\", \"device_name\", \"device_id\", \"cca3\")\n",
    " .as[DeviceTempByCountry]\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c54e5afe",
   "metadata": {},
   "source": [
    "dsTemp.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bea8e54d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------+---------+---------+----+\n",
      "|temp|device_name          |device_id|device_id|cca3|\n",
      "+----+---------------------+---------+---------+----+\n",
      "|34  |meter-gauge-1xbYRYcj |1        |1        |USA |\n",
      "|28  |sensor-pad-4mzWkz    |4        |4        |USA |\n",
      "|27  |sensor-pad-6al7RTAobR|6        |6        |USA |\n",
      "|27  |sensor-pad-8xUD6pzsQI|8        |8        |JPN |\n",
      "|26  |sensor-pad-10BsywSYUF|10       |10       |USA |\n",
      "+----+---------------------+---------+---------+----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dsTemp2: org.apache.spark.sql.Dataset[DeviceTempByCountry] = [temp: bigint, device_name: string ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Otra forma de hacer la query anterior\n",
    "val dsTemp2 = ds\n",
    " .select($\"temp\", $\"device_name\", $\"device_id\", $\"device_id\", $\"cca3\")\n",
    " .where(\"temp > 25\")\n",
    " .as[DeviceTempByCountry]\n",
    "dsTemp2.show(5, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5303a2f",
   "metadata": {},
   "source": [
    "    select() es igual que map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b218c135",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeviceTempByCountry(34,meter-gauge-1xbYRYcj,1,USA)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device: DeviceTempByCountry = DeviceTempByCountry(34,meter-gauge-1xbYRYcj,1,USA)\r\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Miramos solo la primera linea del Dataset\n",
    "val device = dsTemp2.first()\n",
    "println(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b7ae36",
   "metadata": {},
   "source": [
    "**DataFrames vs Datasets**   \n",
    "    - Si quieres decirle a spark que hacer y no como hacerlo -> DF o DS   \n",
    "    - Rica semantica, alto nivel de abstraccion... -> DF o DS   \n",
    "    - Si no importa crear multiples clases -> DS   \n",
    "    - Expresiones de alto nivel, filtros, agregaciones, queries en SQL... ->DF o DS   \n",
    "    - Transformaciones similares a queries de SQL -> DF   \n",
    "    - Unificacion, optimizacion, simplificacion ->DF   \n",
    "    - R o python -> DF    \n",
    "    - Espacio y velocidad eficiente -> DF   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
