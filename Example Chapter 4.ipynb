{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24de1686",
   "metadata": {},
   "source": [
    "1. Hay que crear una base de datos y con el comando USE ponerla en marcha.\n",
    "2. Creamos tablas dentro de la base de datos\n",
    "3. Creamos vistas a partir de las tablas en la base de datos.\n",
    "       Diferencias entre tabla y vista: la vista desaparece cuando cerramos nuestra SparkSession, por el contrario, las tablas continuan creadas en la BBDD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09853775",
   "metadata": {},
   "source": [
    "**Diferencias entre una vista temporal y una vista global temporal**    \n",
    "Una vista temporal solo se puede utilizar en la spark.session en la que se ha creado, por el contrario, una vista temporal global se puede usar en multiples SparkSessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bac2480",
   "metadata": {},
   "source": [
    "Igual que en DF, en SQL tambien podemos establecer la lazy evaluation para las tablas, y que solo sean ejecutadas cuando una accion caiga sobre ellas a traves del siguiente c√≥digo:           \n",
    "\n",
    "    spark.sql(\"CACHE [LAZY] TABLE <table-name>\")\n",
    "    spark.sql(\"UNCACHE TABLE <table-name>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18bbc17",
   "metadata": {},
   "source": [
    "Funcion para leer un fichero y convertirlo en df (opciones de lectura en la pagina 119):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fef8c463",
   "metadata": {},
   "source": [
    "DataFrameReader.format(args).option(\"key\", \"value\").schema(args).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867b1a1",
   "metadata": {},
   "source": [
    "Funcion para guardar un df en un fichero (opciones escritura pag 120):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e264e2c9",
   "metadata": {},
   "source": [
    "DataFrameWriter.format(args)\n",
    " .option(args)\n",
    " .bucketBy(args)\n",
    " .partitionBy(args)\n",
    " .save(path)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa76e684",
   "metadata": {},
   "source": [
    "DataFrameWriter.format(args).option(args).sortBy(args).saveAsTable(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52145029",
   "metadata": {},
   "source": [
    "Para guardar un DataFrame como tabla de SQL utilizamos nombredf.write.saveAsTable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed60974",
   "metadata": {},
   "source": [
    "Tipos de archivos que se pueden leer y escribir:          \n",
    "  - Parquet: guarda datos y metadatos. Tipo de archivo por defecto\n",
    "  - JSON: tipo de archivo facil de leer. Tiene dos formatos: single-line(cada linea tine un objeto JSON) o multiline mode(un unico JSON formado por muchas lineas). Para guardar un DF como JSON hay que establecer la opcion .format(\"json\").option(\"compression\", \"snappy\"). Mas opciones de JSON pag 125.\n",
    "  - CSV: archivos de texto planos, delimitados normalmente por coma, que separa los campos. Mas opciones pag 128.\n",
    "  - Avro: en este formato, al pasar los datos a tabla sql en show hay que establecer 'false'. Mas opciones de abro pag 130.\n",
    "  - Images: pag 132\n",
    "  - Binary files: pag 134"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab349994",
   "metadata": {},
   "source": [
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa401c",
   "metadata": {},
   "source": [
    "### Scala"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2853fc2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://EM2021002836.bosonit.local:4042\n",
       "SparkContext available as 'sc' (version = 3.1.1, master = local[*], app id = local-1623741654944)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c15ac50f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@628b81be\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Creamos una SparkSession\n",
    "val spark = SparkSession\n",
    "    .builder\n",
    "    .appName(\"SparkSQLExampleApp\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9209adba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "csvFile: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Cargamos la ruta de los datos\n",
    "val csvFile= \"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283dbf20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [date: int, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Leemos el fichero e inferimos el esquema de datos\n",
    "val df = spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csvFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d9f55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Creamos una tabla temporal\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecbd630",
   "metadata": {},
   "source": [
    "Para cargar el esquema del DF     \n",
    "val schema = \"date STRING, delay INT, distance INT, \n",
    " origin STRING, destination STRING\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f01e321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 4 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f113f4",
   "metadata": {},
   "source": [
    "Ejemplos con DF vs SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a23b2aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//DF \n",
    "//Opcion 1:\n",
    "df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(col(\"distance\") > 1000)\n",
    " .orderBy(desc(\"distance\")).show(10)\n",
    "\n",
    "//Opcion 2:\n",
    "df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(\"distance > 1000\")\n",
    " .orderBy(desc(\"distance\")).show(10)\n",
    "\n",
    "\n",
    "//SQL\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78cfe2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//DF\n",
    "df.select(\"date\", \"delay\", \"origin\", \"destination\")\n",
    "    .where((col(\"delay\")> 120) and (col(\"origin\")===\"SFO\") and (col(\"destination\")===\"ORD\"))\n",
    "    .orderBy(desc(\"delay\"))\n",
    "    .show(10)\n",
    "\n",
    "//SQL\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b757b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+--------------+\n",
      "|delay|origin|destination|Flights_Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "|  333|   ABE|        ATL|   Long Delays|\n",
      "|  305|   ABE|        ATL|   Long Delays|\n",
      "|  275|   ABE|        ATL|   Long Delays|\n",
      "|  257|   ABE|        ATL|   Long Delays|\n",
      "|  247|   ABE|        DTW|   Long Delays|\n",
      "|  247|   ABE|        ATL|   Long Delays|\n",
      "|  219|   ABE|        ORD|   Long Delays|\n",
      "|  211|   ABE|        ATL|   Long Delays|\n",
      "|  197|   ABE|        DTW|   Long Delays|\n",
      "|  192|   ABE|        ORD|   Long Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//DF\n",
    "df.select(\"delay\", \"origin\", \"destination\")\n",
    "    .withColumn(\"Flights_Delays\", when(col(\"delay\") >360 , \"Very Long Delays\")\n",
    "                                    .when((col(\"delay\") >120) and (col(\"delay\") <360) , \"Long Delays\")\n",
    "                                    .when((col(\"delay\") >60) and  (col(\"delay\") <120), \"Short Delays\")\n",
    "                                    .when((col(\"delay\") >0) and  (col(\"delay\") <60) , \"Tolerable Delays\")\n",
    "                                    .when(col(\"delay\")===0 , \"No Delays\")\n",
    "                                    .otherwise(\"Early\"))\n",
    "     .orderBy(col(\"origin\"), desc(\"delay\"))\n",
    "     .show(10)\n",
    "\n",
    "\n",
    "//SQL\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fa791",
   "metadata": {},
   "source": [
    "Creamos una Base de datos y la ponemos en uso para introducir tablas en ella:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dae57f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf0d121",
   "metadata": {},
   "source": [
    "Creacion de tabla gestionada (cuando eliminamos este tipo de tablas borramos tanto los metadatos como los datos):"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d4a9a9e7",
   "metadata": {},
   "source": [
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9769a305",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Usando la DF API:\n",
    "df.write.saveAsTable(\"managed_us_delay_flights_tbl3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab971d5",
   "metadata": {},
   "source": [
    "Creaci√≥n de tabla no gestionada (cuando eliminamos este tipo de tablas borramos solo los metadatos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962aa458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afc01409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: org.apache.spark.sql.DataFrame = []\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    " distance INT, origin STRING, destination STRING) \n",
    " USING csv OPTIONS (PATH 'C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb6cb408",
   "metadata": {},
   "outputs": [],
   "source": [
    "//Usando la DF API:\n",
    "df\n",
    " .write\n",
    " .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    " .saveAsTable(\"us_delay_flights_tbl4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37437e93",
   "metadata": {},
   "source": [
    "Crear vistas temporales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2dbb8867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "|01011005|   -8|   SFO|        DFW|\n",
      "|01011800|    0|   SFO|        ORD|\n",
      "|01011740|   -7|   SFO|        LAX|\n",
      "|01012015|   -7|   SFO|        LAX|\n",
      "|01012110|   -1|   SFO|        MIA|\n",
      "|01011610|  134|   SFO|        DFW|\n",
      "|01011240|   -6|   SFO|        MIA|\n",
      "|01010755|   -3|   SFO|        DFW|\n",
      "|01010020|    0|   SFO|        DFW|\n",
      "|01010705|   -6|   SFO|        LAX|\n",
      "|01010925|   -3|   SFO|        ORD|\n",
      "|01010555|   -6|   SFO|        ORD|\n",
      "|01011105|   -8|   SFO|        DFW|\n",
      "|01012330|   32|   SFO|        ORD|\n",
      "|01011330|    3|   SFO|        DFW|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO' \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "621d4458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_sfo: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//API DF\n",
    "val df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view2\")\n",
    "df_sfo.show(5)            \n",
    "    \n",
    "//SQL\n",
    "spark.sql(\"\"\"CREATE TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO'\"\"\")\n",
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from us_origin_airport_SFO_global_tmp_view\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96554205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_jfk: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 2 more fields]\r\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//API DF\n",
    "val df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view2\")\n",
    "df_jfk.show(5)\n",
    "\n",
    "//SQL\n",
    "spark.sql(\"\"\"CREATE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\"\"\")\n",
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from us_origin_airport_JFK_tmp_view\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108c6e2b",
   "metadata": {},
   "source": [
    "Cuando hacemos una vista global, para llamar a dicha vista, colocaremos delante del nommbre 'global_temp.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90254b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE GLOBAL TEMP VIEW prueba AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\"\"\")\n",
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from global_temp.prueba\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21d65f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n",
      "|      database|           tableName|isTemporary|\n",
      "+--------------+--------------------+-----------+\n",
      "|learn_spark_db|us_delay_flights_tbl|      false|\n",
      "|learn_spark_db|us_delay_flights_...|      false|\n",
      "|              |us_origin_airport...|       true|\n",
      "|              |us_origin_airport...|       true|\n",
      "|              |us_origin_airport...|       true|\n",
      "+--------------+--------------------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bda7554",
   "metadata": {},
   "source": [
    "Para borrar vistas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d16cea07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res20: Boolean = false\r\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//SQL\n",
    "spark.sql(\"\"\"DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\"\"\")\n",
    "spark.sql(\"\"\"DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\"\"\")\n",
    "\n",
    "\n",
    "spark.catalog.dropGlobalTempView(\"prueba\") //vista global \n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588b5217",
   "metadata": {},
   "source": [
    "Acceder a los metadatos de una base de datos, tabla o columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e9498aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------------------+\n",
      "|          name|     description|         locationUri|\n",
      "+--------------+----------------+--------------------+\n",
      "|       default|default database|file:/C:/Users/ne...|\n",
      "|learn_spark_db|                |file:/C:/Users/ne...|\n",
      "+--------------+----------------+--------------------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listDatabases().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f59ff918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+-----------+---------+-----------+\n",
      "|                name|      database|description|tableType|isTemporary|\n",
      "+--------------------+--------------+-----------+---------+-----------+\n",
      "|us_delay_flights_tbl|learn_spark_db|       null| EXTERNAL|      false|\n",
      "|us_delay_flights_...|learn_spark_db|       null| EXTERNAL|      false|\n",
      "|us_origin_airport...|          null|       null|TEMPORARY|       true|\n",
      "+--------------------+--------------+-----------+---------+-----------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listTables().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f070d773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "|       name|description|dataType|nullable|isPartition|isBucket|\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "|       date|       null|  string|    true|      false|   false|\n",
      "|      delay|       null|     int|    true|      false|   false|\n",
      "|   distance|       null|     int|    true|      false|   false|\n",
      "|     origin|       null|  string|    true|      false|   false|\n",
      "|destination|       null|  string|    true|      false|   false|\n",
      "+-----------+-----------+--------+--------+-----------+--------+\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85acd280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usFlightsDF: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n",
       "usFlightsDF2: org.apache.spark.sql.DataFrame = [date: string, delay: int ... 3 more fields]\r\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Guardamos una tabla de SQL en DF\n",
    "val usFlightsDF = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "val usFlightsDF2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80757503",
   "metadata": {},
   "source": [
    "Importar ficheros a DF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e59f918f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/2010-summary.parquet\r\n",
       "df: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n",
       "df2: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use Parquet \n",
    "val file =\"\"\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/2010-summary.parquet\"\"\"\n",
    "val df = spark.read.format(\"parquet\").load(file)\n",
    "// Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "val df2 = spark.read.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0548110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df3: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use CSV\n",
    "val df3 = spark.read.format(\"csv\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"mode\", \"PERMISSIVE\")\n",
    " .load(\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9008065b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df4: org.apache.spark.sql.DataFrame = [DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Use JSON\n",
    "val df4 = spark.read.format(\"json\")\n",
    " .load(\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/json/*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c9633",
   "metadata": {},
   "source": [
    "Exportar DF a fichero:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4da74311",
   "metadata": {},
   "source": [
    "// Use JSON\n",
    "val location = ...\n",
    "df.write.format(\"json\").mode(\"overwrite\").save(location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4cbdcc",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3be2261e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1619f167",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creamos la SparkSession\n",
    "spark =(SparkSession\n",
    "        .builder\n",
    "        .appName(\"SparkSQLExampleApp\")\n",
    "        .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87239067",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cargamos la ruta de los datos\n",
    "csv_file =\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38706cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##leemos los datos\n",
    "df = (spark.read.format(\"csv\")\n",
    "     .option(\"header\", \"true\")\n",
    "     .option(\"inferSchema\", \"true\")\n",
    "     .load(csv_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d3e2577",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creamos una tabla temporal\n",
    "df.createOrReplaceTempView(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9a5e15",
   "metadata": {},
   "source": [
    "Para cargar el esquema del DF      \n",
    "        schema = \"`date` STRING, `delay` INT, `distance` INT, \n",
    "`origin` STRING, `destination` STRING\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36aad215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\").show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6968962b",
   "metadata": {},
   "source": [
    "Ejemplos con DF vs SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d716a0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "|    4330|   JFK|        HNL|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+--------+------+-----------+\n",
      "|distance|origin|destination|\n",
      "+--------+------+-----------+\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "|    4330|   HNL|        JFK|\n",
      "+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "##DF\n",
    "##Opcion 1\n",
    "from pyspark.sql.functions import col, desc\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\\\n",
    " .where(col(\"distance\") > 1000)\\\n",
    " .orderBy(desc(\"distance\"))).show(10)\n",
    "\n",
    "##Opcion 2\n",
    "(df.select(\"distance\", \"origin\", \"destination\")\n",
    " .where(\"distance > 1000\")\n",
    " .orderBy(\"distance\", ascending=False).show(10))\n",
    "\n",
    "\n",
    "##SQL\n",
    "spark.sql(\"\"\"SELECT distance, origin, destination \n",
    "FROM us_delay_flights_tbl WHERE distance > 1000 \n",
    "ORDER BY distance DESC\"\"\").show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb5b55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------+-----+------+-----------+\n",
      "|   date|delay|origin|destination|\n",
      "+-------+-----+------+-----------+\n",
      "|2190925| 1638|   SFO|        ORD|\n",
      "|1031755|  396|   SFO|        ORD|\n",
      "|1022330|  326|   SFO|        ORD|\n",
      "|1051205|  320|   SFO|        ORD|\n",
      "|1190925|  297|   SFO|        ORD|\n",
      "|2171115|  296|   SFO|        ORD|\n",
      "|1071040|  279|   SFO|        ORD|\n",
      "|1051550|  274|   SFO|        ORD|\n",
      "|3120730|  266|   SFO|        ORD|\n",
      "|1261104|  258|   SFO|        ORD|\n",
      "+-------+-----+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##DF\n",
    "(df.select(\"date\", \"delay\", \"origin\", \"destination\")\\\n",
    "    .where((col(\"delay\")> 120) & (col(\"origin\")=='SFO') & (col(\"destination\")=='ORD'))\\\n",
    "    .orderBy(desc(\"delay\"))\\\n",
    "    .show(10))\n",
    "\n",
    "##SQL\n",
    "spark.sql(\"\"\"SELECT date, delay, origin, destination \n",
    "FROM us_delay_flights_tbl \n",
    "WHERE delay > 120 AND ORIGIN = 'SFO' AND DESTINATION = 'ORD' \n",
    "ORDER by delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00c9c459",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----------+--------------+\n",
      "|delay|origin|destination|Flights_Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "|  333|   ABE|        ATL|   Long Delays|\n",
      "|  305|   ABE|        ATL|   Long Delays|\n",
      "|  275|   ABE|        ATL|   Long Delays|\n",
      "|  257|   ABE|        ATL|   Long Delays|\n",
      "|  247|   ABE|        ATL|   Long Delays|\n",
      "|  247|   ABE|        DTW|   Long Delays|\n",
      "|  219|   ABE|        ORD|   Long Delays|\n",
      "|  211|   ABE|        ATL|   Long Delays|\n",
      "|  197|   ABE|        DTW|   Long Delays|\n",
      "|  192|   ABE|        ORD|   Long Delays|\n",
      "+-----+------+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----+------+-----------+-------------+\n",
      "|delay|origin|destination|Flight_Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "|  333|   ABE|        ATL|  Long Delays|\n",
      "|  305|   ABE|        ATL|  Long Delays|\n",
      "|  275|   ABE|        ATL|  Long Delays|\n",
      "|  257|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        ATL|  Long Delays|\n",
      "|  247|   ABE|        DTW|  Long Delays|\n",
      "|  219|   ABE|        ORD|  Long Delays|\n",
      "|  211|   ABE|        ATL|  Long Delays|\n",
      "|  197|   ABE|        DTW|  Long Delays|\n",
      "|  192|   ABE|        ORD|  Long Delays|\n",
      "+-----+------+-----------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##DF\n",
    "(df.select(\"delay\", \"origin\", \"destination\")\n",
    "    .withColumn(\"Flights_Delays\", F.when(F.col(\"delay\") >360 , \"Very Long Delays\")\\\n",
    "                                    .when((F.col(\"delay\") >120) &  (F.col(\"delay\") <360) , \"Long Delays\")\\\n",
    "                                    .when((F.col(\"delay\") >60) &  (F.col(\"delay\") <120), \"Short Delays\")\\\n",
    "                                    .when((F.col(\"delay\") >0) &  (F.col(\"delay\") <60) , \"Tolerable Delays\")\\\n",
    "                                    .when(F.col(\"delay\")==0 , \"No Delays\")\\\n",
    "                                    .otherwise(\"Early\"))\\\n",
    "     .orderBy(\"origin\", desc(\"delay\"))\\\n",
    "     .show(10))\n",
    "\n",
    "\n",
    "##SQL\n",
    "spark.sql(\"\"\"SELECT delay, origin, destination,\n",
    " CASE\n",
    " WHEN delay > 360 THEN 'Very Long Delays'\n",
    " WHEN delay > 120 AND delay < 360 THEN 'Long Delays'\n",
    " WHEN delay > 60 AND delay < 120 THEN 'Short Delays'\n",
    " WHEN delay > 0 and delay < 60 THEN 'Tolerable Delays'\n",
    " WHEN delay = 0 THEN 'No Delays'\n",
    " ELSE 'Early'\n",
    " END AS Flight_Delays\n",
    " FROM us_delay_flights_tbl\n",
    " ORDER BY origin, delay DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be28a38",
   "metadata": {},
   "source": [
    "Creamos una Base de datos y la ponemos en uso para introducir tablas en ella:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6952f66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP DATABASE learn_spark_db CASCADE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e48006e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE learn_spark_db\")\n",
    "spark.sql(\"USE learn_spark_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2976c",
   "metadata": {},
   "source": [
    "Creacion de tabla gestionada (cuando eliminamos este tipo de tablas borramos tanto los metadatos como los datos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3c6a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE TABLE managed_us_delay_flights_tbl2 (date STRING, delay INT, distance INT, origin STRING, destination STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d740bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Usando la DF API:\n",
    "df.write.saveAsTable(\"managed_us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9241f15",
   "metadata": {},
   "source": [
    "Creaci√≥n de tabla no gestionada (cuando eliminamos este tipo de tablas borramos solo los metadatos):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "896ae440",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2de56930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE TABLE us_delay_flights_tbl(date STRING, delay INT, \n",
    " distance INT, origin STRING, destination STRING) \n",
    " USING csv OPTIONS (PATH 'C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/departuredelays.csv')\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc8b9b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Usando la DF API:\n",
    "(df\n",
    " .write\n",
    " .option(\"path\", \"/tmp/data/us_flights_delay\")\n",
    " .saveAsTable(\"us_delay_flights_tbl2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010cdc15",
   "metadata": {},
   "source": [
    "Crear vistas temporales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc942c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "|01011005|   -8|   SFO|        DFW|\n",
      "|01011800|    0|   SFO|        ORD|\n",
      "|01011740|   -7|   SFO|        LAX|\n",
      "|01012015|   -7|   SFO|        LAX|\n",
      "|01012110|   -1|   SFO|        MIA|\n",
      "|01011610|  134|   SFO|        DFW|\n",
      "|01011240|   -6|   SFO|        MIA|\n",
      "|01010755|   -3|   SFO|        DFW|\n",
      "|01010020|    0|   SFO|        DFW|\n",
      "|01010705|   -6|   SFO|        LAX|\n",
      "|01010925|   -3|   SFO|        ORD|\n",
      "|01010555|   -6|   SFO|        ORD|\n",
      "|01011105|   -8|   SFO|        DFW|\n",
      "|01012330|   32|   SFO|        ORD|\n",
      "|01011330|    3|   SFO|        DFW|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO' \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8366a181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01011250|   55|   SFO|        JFK|\n",
      "|01012230|    0|   SFO|        JFK|\n",
      "|01010705|   -7|   SFO|        JFK|\n",
      "|01010620|   -3|   SFO|        MIA|\n",
      "|01010915|   -3|   SFO|        LAX|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##API DF\n",
    "df_sfo = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'\")\n",
    "df_sfo.createOrReplaceGlobalTempView(\"us_origin_airport_SFO_global_tmp_view2\")\n",
    "df_sfo.show(5)            \n",
    "    \n",
    "#SQL\n",
    "spark.sql(\"\"\"CREATE TEMP VIEW us_origin_airport_SFO_global_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'SFO'\"\"\")\n",
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from us_origin_airport_SFO_global_tmp_view\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d903a91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##API DF\n",
    "df_jfk = spark.sql(\"SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'\")\n",
    "df_jfk.createOrReplaceTempView(\"us_origin_airport_JFK_tmp_view2\")\n",
    "df_jfk.show(5)\n",
    "\n",
    "##SQL\n",
    "spark.sql(\"\"\"CREATE TEMP VIEW us_origin_airport_JFK_tmp_view AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\"\"\")\n",
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from us_origin_airport_JFK_tmp_view\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c334ed",
   "metadata": {},
   "source": [
    "Cuando hacemos una vista global, para llamar a dicha vista, colocaremos delante del nommbre 'global_temp.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e56bf55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+-----------+\n",
      "|    date|delay|origin|destination|\n",
      "+--------+-----+------+-----------+\n",
      "|01010900|   14|   JFK|        LAX|\n",
      "|01011200|   -3|   JFK|        LAX|\n",
      "|01011900|    2|   JFK|        LAX|\n",
      "|01011700|   11|   JFK|        LAS|\n",
      "|01010800|   -1|   JFK|        SFO|\n",
      "+--------+-----+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"CREATE GLOBAL TEMP VIEW prueba AS\n",
    " SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE\n",
    " origin = 'JFK'\"\"\")\n",
    "spark.sql(\"\"\" SELECT date, delay, origin, destination from global_temp.prueba\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c91602c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+-----------+\n",
      "|      database|           tableName|isTemporary|\n",
      "+--------------+--------------------+-----------+\n",
      "|learn_spark_db|managed_us_delay_...|      false|\n",
      "|learn_spark_db|managed_us_delay_...|      false|\n",
      "|learn_spark_db|us_delay_flights_tbl|      false|\n",
      "|learn_spark_db|us_delay_flights_...|      false|\n",
      "|              |us_origin_airport...|       true|\n",
      "|              |us_origin_airport...|       true|\n",
      "|              |us_origin_airport...|       true|\n",
      "+--------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SHOW TABLES\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7336abbd",
   "metadata": {},
   "source": [
    "Para borrar vistas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa66a0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##SQL\n",
    "spark.sql(\"\"\"DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view;\"\"\")\n",
    "spark.sql(\"\"\"DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view\"\"\")\n",
    "\n",
    "#en pyspark\n",
    "spark.catalog.dropGlobalTempView(\"prueba\") #vista global \n",
    "spark.catalog.dropTempView(\"us_origin_airport_JFK_tmp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fb17fa",
   "metadata": {},
   "source": [
    "Acceder a los metadatos de una base de datos, tabla o columnas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "192f857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='file:/C:/Users/nerea.gomez/Documents/Documentacion/Learning%20Spark/spark-warehouse'),\n",
       " Database(name='learn_spark_db', description='', locationUri='file:/C:/Users/nerea.gomez/Documents/Documentacion/Learning%20Spark/spark-warehouse/learn_spark_db.db')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e622b93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='managed_us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='managed_us_delay_flights_tbl2', database='learn_spark_db', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_delay_flights_tbl2', database='learn_spark_db', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='us_origin_airport_jfk_tmp_view2', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f12d732b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='date', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='delay', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='distance', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='origin', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='destination', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listColumns(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "828b3404",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Guardamos una tabla de SQL en DF\n",
    "us_flights_df = spark.sql(\"SELECT * FROM us_delay_flights_tbl\")\n",
    "us_flights_df2 = spark.table(\"us_delay_flights_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86bbbb9",
   "metadata": {},
   "source": [
    "Lectura de ficheros con diferentes extensiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db528b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use Parquet \n",
    "file = \"\"\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/2010-summary.parquet\"\"\"\n",
    "df = spark.read.format(\"parquet\").load(file)\n",
    "## Use Parquet; you can omit format(\"parquet\") if you wish as it's the default\n",
    "df2 = spark.read.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "95965542",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use CSV\n",
    "df3 = spark.read.format(\"csv\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").option(\"mode\", \"PERMISSIVE\")\\\n",
    "            .load(\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/csv/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94fc114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use JSON\n",
    "df4 = spark.read.format(\"json\").load(\"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/json/*\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
