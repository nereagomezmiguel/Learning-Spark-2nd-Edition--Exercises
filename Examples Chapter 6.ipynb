{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad50973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "import org.apache.spark.sql.types._\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.types._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20b80903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Id,IntegerType,false), StructField(First,StringType,false), StructField(Last,StringType,false), StructField(Url,StringType,false), StructField(Published,StringType,false), StructField(Hits,IntegerType,false), StructField(Campaigns,ArrayType(StringType,true),false))\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " val schema = StructType(Array(StructField(\"Id\", IntegerType, false),\n",
    "     StructField(\"First\", StringType, false),\n",
    "     StructField(\"Last\", StringType, false),\n",
    "     StructField(\"Url\", StringType, false),\n",
    "     StructField(\"Published\", StringType, false),\n",
    "     StructField(\"Hits\", IntegerType, false),\n",
    "     StructField(\"Campaigns\", ArrayType(StringType), false)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c85e6845",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bloggers: String = C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/blogs.json\r\n",
       "bloggersDS: org.apache.spark.sql.DataFrame = [Id: int, First: string ... 5 more fields]\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val bloggers = \"C:/Users/nerea.gomez/Documents/Documentacion/Learning Spark/Datasets/blogs.json\"\n",
    "val bloggersDS = spark.read.schema(schema).format(\"json\").load(bloggers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e4bfa",
   "metadata": {},
   "source": [
    "**Creating Sample Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93ab1e5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.util.Random._\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scala.util.Random._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8004ed0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Usage\r\n",
       "r: scala.util.Random = scala.util.Random@3ba8552b\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Usage(uid:Int, uname:String, usage: Int)\n",
    "val r = new scala.util.Random(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2edb98d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data: scala.collection.immutable.IndexedSeq[Usage] = Vector(Usage(0,user-Gpi2C,525), Usage(1,user-DgXDi,502), Usage(2,user-M66yO,170), Usage(3,user-xTOn6,913), Usage(4,user-3xGSz,246), Usage(5,user-2aWRN,727), Usage(6,user-EzZY1,65), Usage(7,user-ZlZMZ,935), Usage(8,user-VjxeG,756), Usage(9,user-iqf1P,3), Usage(10,user-91S1q,794), Usage(11,user-qHNj0,501), Usage(12,user-7hb94,460), Usage(13,user-bz0WF,142), Usage(14,user-71nwy,479), Usage(15,user-7GZz1,823), Usage(16,user-1CSk6,140), Usage(17,user-WPzlL,246), Usage(18,user-VaEit,451), Usage(19,user-PSaRq,679), Usage(20,user-0Kkzu,332), Usage(21,user-UN3MG,172), Usage(22,user-KwwER,442), Usage(23,user-ZnltJ,923), Usage(24,user-IRA17,741), Usage(25,user-yNHRT,299), Usage(26,user-CJY3C,996), Usage(27,user-Yq9WW,529), Usage(28,user-RFWw1,30...\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = for (i <- 0 to 1000)\n",
    " yield (Usage(i, \"user-\" + r.alphanumeric.take(5).mkString(\"\"),\n",
    " r.nextInt(1000)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8438b555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|     uname|usage|\n",
      "+---+----------+-----+\n",
      "|  0|user-Gpi2C|  525|\n",
      "|  1|user-DgXDi|  502|\n",
      "|  2|user-M66yO|  170|\n",
      "|  3|user-xTOn6|  913|\n",
      "|  4|user-3xGSz|  246|\n",
      "|  5|user-2aWRN|  727|\n",
      "|  6|user-EzZY1|   65|\n",
      "|  7|user-ZlZMZ|  935|\n",
      "|  8|user-VjxeG|  756|\n",
      "|  9|user-iqf1P|    3|\n",
      "+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dsUsage: org.apache.spark.sql.Dataset[Usage] = [uid: int, uname: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dsUsage = spark.createDataset(data)\n",
    "dsUsage.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0799201",
   "metadata": {},
   "source": [
    "**Higher-order functions and functional programming**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05bb8a96",
   "metadata": {},
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "dsUsage\n",
    " .filter(d => d.usage > 900)\n",
    " .orderBy(desc(\"usage\"))\n",
    " .show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "492fd530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----+\n",
      "|uid|uname     |usage|\n",
      "+---+----------+-----+\n",
      "|113|user-nnAXr|999  |\n",
      "|634|user-L0wci|999  |\n",
      "|605|user-NL6c4|999  |\n",
      "|561|user-5n2xY|999  |\n",
      "|26 |user-CJY3C|996  |\n",
      "+---+----------+-----+\n",
      "only showing top 5 rows\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "dsUsage.select(\"*\").filter(col(\"usage\")>900).orderBy(desc(\"usage\")).show(5, false)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8a7a513",
   "metadata": {},
   "source": [
    "// Use an if-then-else lambda expression and compute a value\n",
    "dsUsage.map(u => {if (u.usage > 750) u.usage * .15 else u.usage * .50 }).show(5, false)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d877704",
   "metadata": {},
   "source": [
    "// Define a function to compute the usage\n",
    "def computeCostUsage(usage: Int): Double = {\n",
    " if (usage > 750) usage * 0.15 else usage * 0.50\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "93eeaf33",
   "metadata": {},
   "source": [
    "// Use the function as an argument to map()\n",
    "dsUsage.map(u => {computeCostUsage(u.usage)}).show(5, false)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0a1bded7",
   "metadata": {},
   "source": [
    "// Create a new case class with an additional field, cost\n",
    "case class UsageCost(uid: Int, uname:String, usage: Int, cost: Double)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "68bd1ce7",
   "metadata": {},
   "source": [
    "// Compute the usage cost with Usage as a parameter\n",
    "// Return a new object, UsageCost\n",
    "def computeUserCostUsage(u: Usage): UsageCost = {\n",
    " val v = if (u.usage > 750) u.usage * 0.15 else u.usage * 0.50\n",
    " UsageCost(u.uid, u.uname, u.usage, v)\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9f6fdf0",
   "metadata": {},
   "source": [
    "// Use map() on our original Dataset\n",
    "dsUsage.map(u => {computeUserCostUsage(u)}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7c0186",
   "metadata": {},
   "source": [
    "Observaciones de usar higher-order functions and Datasets:\n",
    "- Usan un tipo de objeto JVM como argumento de la funcion.\n",
    "- Utilizamos notacion de puntos (programacion orientada a objetos) para acceder a los campos individuales dentro del objeto JVM\n",
    "- Algunas de las funciones lambda son de tipo seguro, detctanto los errores en tiempo de compilacion e indicado a Spark sobre que tipos de datos trabajar, que operaciones realizar, etc.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
